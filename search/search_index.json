{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CosmoPower is a library for Machine Learning - accelerated Bayesian inference. While the emphasis is on building algorithms to accelerate Bayesian inference in cosmology , the interdisciplinary nature of the methodologies implemented in the package allows for their application across a wide range of scientific fields. The ultimate goal of CosmoPower is to solve inverse problems in science, by developing Bayesian inference pipelines that leverage the computational power of Machine Learning to accelerate the inference process. This approach represents a principled application of Machine Learning to scientific research, with the Machine Learning component embedded within a rigorous framework for uncertainty quantification. In cosmology, CosmoPower aims to become a fully differentiable library for cosmological analyses. Currently, CosmoPower provides neural network emulators of matter and Cosmic Microwave Background power spectra. These emulators can be used to replace Boltzmann codes such as CAMB or CLASS in cosmological inference pipelines, to source the power spectra needed for two-point statistics analyses. This provides orders-of-magnitude acceleration to the inference pipeline and integrates naturally with efficient techniques for sampling very high-dimensional parameter spaces. The power spectra emulators implemented in CosmoPower, and first presented in its release paper , have been applied to the analysis of real cosmological data from experiments, as well as having been tested against the accuracy requirements for the analysis of next-generation cosmological surveys. CosmoPower is written entirely in Python . Neural networks are implemented using the TensorFlow library.","title":"Home"},{"location":"citation/","text":"If you use CosmoPower at any point in your work please cite its release paper : @article{spuriomancini2021, title={CosmoPower: emulating cosmological power spectra for accelerated Bayesian inference from next-generation surveys}, author={{Spurio Mancini}, A. and {Piras}, D. and {Alsing}, J. and {Joachimi}, B. and {Hobson}, M.~P.}, year={2021}, eprint={2106.03846}, archivePrefix={arXiv}, primaryClass={astro-ph.CO} } If you use a specific likelihood or trained model then in addition to the release paper please also cite their relevant papers (always listed in the corresponding directory).","title":"Citation"},{"location":"contribute/","text":"For bugs and feature requests consider using the issue tracker . Contributions to the code via pull requests are most welcome! For general support, please send an email to a dot spuriomancini at ucl dot ac dot uk , or post on GitHub discussions . Users of CosmoPower are strongly encouraged to join the GitHub discussions forum to follow the latest news on the code as well as to discuss all things Machine Learning / Bayesian Inference in cosmology!","title":"Contribute/Support/Community"},{"location":"installation/","text":"Installation \u00b6 We recommend installing CosmoPower within a Conda virtual environment. For example, to create and activate an environment called cp_env , use: conda create -n cp_env python=3.7 pip && conda activate cp_env Once inside the environment, you can install CosmoPower : from PyPI pip install cosmopower To test the installation, you can use python3 -c 'import cosmopower as cp' If you do not have a GPU on your machine, you will see a warning message about it which you can safely ignore. from source git clone https://github.com/alessiospuriomancini/cosmopower cd cosmopower pip install . To test the installation, you can use pytest","title":"Installation"},{"location":"installation/#installation","text":"We recommend installing CosmoPower within a Conda virtual environment. For example, to create and activate an environment called cp_env , use: conda create -n cp_env python=3.7 pip && conda activate cp_env Once inside the environment, you can install CosmoPower : from PyPI pip install cosmopower To test the installation, you can use python3 -c 'import cosmopower as cp' If you do not have a GPU on your machine, you will see a warning message about it which you can safely ignore. from source git clone https://github.com/alessiospuriomancini/cosmopower cd cosmopower pip install . To test the installation, you can use pytest","title":"Installation"},{"location":"API/cosmopower_NN-reference/","text":"cosmopower_NN module \u00b6 cosmopower_NN ( Model ) \u00b6 Mapping between cosmological parameters and (log)-power spectra Attributes: Name Type Description parameters list [str] model parameters, sorted in the desired order modes numpy.ndarray multipoles or k-values in the (log)-spectra parameters_mean numpy.ndarray mean of input parameters parameters_std numpy.ndarray std of input parameters features_mean numpy.ndarray mean of output features features_std numpy.ndarray std of output features n_hidden list [int] number of nodes for each hidden layer restore bool whether to restore a previously trained model or not restore_filename str filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable bool training layers optimizer tf.keras.optimizers optimizer for training verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_NN.py class cosmopower_NN ( tf . keras . Model ): r \"\"\" Mapping between cosmological parameters and (log)-power spectra Attributes: parameters (list [str]): model parameters, sorted in the desired order modes (numpy.ndarray): multipoles or k-values in the (log)-spectra parameters_mean (numpy.ndarray): mean of input parameters parameters_std (numpy.ndarray): std of input parameters features_mean (numpy.ndarray): mean of output features features_std (numpy.ndarray): std of output features n_hidden (list [int]): number of nodes for each hidden layer restore (bool): whether to restore a previously trained model or not restore_filename (str): filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable (bool): training layers optimizer (tf.keras.optimizers): optimizer for training verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , parameters = None , modes = None , parameters_mean = None , parameters_std = None , features_mean = None , features_std = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): \"\"\" Constructor \"\"\" # super super ( cosmopower_NN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # attributes self . parameters = parameters self . n_parameters = len ( self . parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_modes ] self . n_layers = len ( self . architecture ) - 1 # input parameters mean and std self . parameters_mean_ = parameters_mean if parameters_mean is not None else np . zeros ( self . n_parameters ) self . parameters_std_ = parameters_std if parameters_std is not None else np . ones ( self . n_parameters ) # (log)-spectra mean and std self . features_mean_ = features_mean if features_mean is not None else np . zeros ( self . n_modes ) self . features_std_ = features_std if features_std is not None else np . ones ( self . n_modes ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , 1e-3 ), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) # optimizer self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_NN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_modes } output modes, \\n \" \\ f \"using { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) # ========== TENSORFLOW implementation =============== # non-linear activation function def activation ( self , x , alpha , beta ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) # tensor predictions @tf . function def predictions_tf ( self , parameters_tensor ): r \"\"\" Prediction given tensor of input parameters, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale -> output predictions return tf . add ( tf . multiply ( layers [ - 1 ], self . features_std ), self . features_mean ) # tensor 10.**predictions @tf . function def ten_to_predictions_tf ( self , parameters_tensor ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_tensor )) # ============= SAVE/LOAD model ============= # save network parameters to Numpy arrays def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put mean and std parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy () # save def save ( self , filename ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . features_mean_ , self . features_std_ , self . n_parameters , self . parameters , self . n_modes , self . modes , self . n_hidden , self . n_layers , self . architecture ] # save attributes to file with open ( filename + \".pkl\" , 'wb' ) as f : pickle . dump ( attributes , f ) # restore attributes def restore ( self , filename ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes with open ( filename + \".pkl\" , 'rb' ) as f : self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . n_parameters , self . parameters , \\ self . n_modes , self . modes , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) # ========== NUMPY implementation =============== # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # forward prediction given input parameters implemented in Numpy def forward_pass_np ( self , parameters_arr ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (standardised) predictions layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale and output return layers [ - 1 ] * self . features_std_ + self . features_mean_ # Numpy array predictions def predictions_np ( self , parameters_dict ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) # Numpy array 10.**predictions def ten_to_predictions_np ( self , parameters_dict ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ### Infrastructure for network training ### @tf . function def compute_loss ( self , training_parameters , training_features ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) @tf . function def compute_loss_and_gradients ( self , training_parameters , training_features ): r \"\"\" Computes mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients def training_step ( self , training_parameters , training_features ): r \"\"\" Minimize loss Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_features ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss def training_step_with_accumulated_gradients ( self , training_parameters , training_features , accumulation_steps = 10 ): r \"\"\" Minimize loss, breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_features )) . batch ( int ( training_features . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_features_ , in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters_ , training_features_ ) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss # ========================================== # main TRAINING function # ========================================== def train ( self , training_parameters , training_features , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: training_parameters (dict [numpy.ndarray]): input parameters training_features (numpy.ndarray): true features for training filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_NN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # from dict to array training_parameters = self . dict_to_ordered_arr_np ( training_parameters ) # parameters standardisation self . parameters_mean = np . mean ( training_parameters , axis = 0 ) self . parameters_std = np . std ( training_parameters , axis = 0 ) # features standardisation self . features_mean = np . mean ( training_features , axis = 0 ) self . features_std = np . std ( training_features , axis = 0 ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std , dtype = dtype , name = 'features_std' ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # casting training_parameters = tf . convert_to_tensor ( training_parameters , dtype = dtype ) training_features = tf . convert_to_tensor ( training_features , dtype = dtype ) # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_features [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , feats in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , feats ) else : loss = self . training_step_with_accumulated_gradients ( theta , feats , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_features [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) __init__ ( self , parameters = None , modes = None , parameters_mean = None , parameters_std = None , features_mean = None , features_std = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer =< keras . optimizer_v2 . adam . Adam object at 0x7f9f759f6b10 > , verbose = False ) special \u00b6 Constructor Source code in cosmopower/cosmopower_NN.py def __init__ ( self , parameters = None , modes = None , parameters_mean = None , parameters_std = None , features_mean = None , features_std = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): \"\"\" Constructor \"\"\" # super super ( cosmopower_NN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # attributes self . parameters = parameters self . n_parameters = len ( self . parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_modes ] self . n_layers = len ( self . architecture ) - 1 # input parameters mean and std self . parameters_mean_ = parameters_mean if parameters_mean is not None else np . zeros ( self . n_parameters ) self . parameters_std_ = parameters_std if parameters_std is not None else np . ones ( self . n_parameters ) # (log)-spectra mean and std self . features_mean_ = features_mean if features_mean is not None else np . zeros ( self . n_modes ) self . features_std_ = features_std if features_std is not None else np . ones ( self . n_modes ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , 1e-3 ), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) # optimizer self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_NN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_modes } output modes, \\n \" \\ f \"using { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) activation ( self , x , alpha , beta ) \u00b6 Non-linear activation function Parameters: Name Type Description Default x Tensor linear output from previous layer required alpha Tensor trainable parameter required beta Tensor trainable parameter required Returns: Type Description Tensor the result of applying the non-linear activation function to the linear output of the layer Source code in cosmopower/cosmopower_NN.py def activation ( self , x , alpha , beta ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) compute_loss ( self , training_parameters , training_features ) \u00b6 Mean squared difference Parameters: Name Type Description Default training_parameters Tensor input parameters required training_features Tensor true features required Returns: Type Description Tensor mean squared difference Source code in cosmopower/cosmopower_NN.py @tf . function def compute_loss ( self , training_parameters , training_features ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) compute_loss_and_gradients ( self , training_parameters , training_features ) \u00b6 Computes mean squared difference and gradients Parameters: Name Type Description Default training_parameters Tensor input parameters required training_features Tensor true features required Returns: Type Description loss (Tensor) mean squared difference gradients (Tensor): gradients Source code in cosmopower/cosmopower_NN.py @tf . function def compute_loss_and_gradients ( self , training_parameters , training_features ): r \"\"\" Computes mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients dict_to_ordered_arr_np ( self , input_dict ) \u00b6 Sort input parameters Parameters: Name Type Description Default input_dict dict [numpy.ndarray] input dict of (arrays of) parameters to be sorted required Returns: Type Description numpy.ndarray parameters sorted according to desired order Source code in cosmopower/cosmopower_NN.py def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) forward_pass_np ( self , parameters_arr ) \u00b6 Forward pass through the network to predict the output, fully implemented in Numpy Parameters: Name Type Description Default parameters_arr numpy.ndarray array of input parameters required Returns: Type Description numpy.ndarray output predictions Source code in cosmopower/cosmopower_NN.py def forward_pass_np ( self , parameters_arr ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (standardised) predictions layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale and output return layers [ - 1 ] * self . features_std_ + self . features_mean_ predictions_np ( self , parameters_dict ) \u00b6 Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls forward_pass_np after ordering the input parameter dict Parameters: Name Type Description Default parameters_dict dict [numpy.ndarray] dictionary of (arrays of) parameters required Returns: Type Description numpy.ndarray output predictions Source code in cosmopower/cosmopower_NN.py def predictions_np ( self , parameters_dict ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) predictions_tf ( self , parameters_tensor ) \u00b6 Prediction given tensor of input parameters, fully implemented in TensorFlow Parameters: Name Type Description Default parameters_tensor Tensor input parameters required Returns: Type Description Tensor output predictions Source code in cosmopower/cosmopower_NN.py @tf . function def predictions_tf ( self , parameters_tensor ): r \"\"\" Prediction given tensor of input parameters, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale -> output predictions return tf . add ( tf . multiply ( layers [ - 1 ], self . features_std ), self . features_mean ) restore ( self , filename ) \u00b6 Load pre-trained model Parameters: Name Type Description Default filename str filename tag (without suffix) where model was saved required Source code in cosmopower/cosmopower_NN.py def restore ( self , filename ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes with open ( filename + \".pkl\" , 'rb' ) as f : self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . n_parameters , self . parameters , \\ self . n_modes , self . modes , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) save ( self , filename ) \u00b6 Save network parameters Parameters: Name Type Description Default filename str filename tag (without suffix) where model will be saved required Source code in cosmopower/cosmopower_NN.py def save ( self , filename ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . features_mean_ , self . features_std_ , self . n_parameters , self . parameters , self . n_modes , self . modes , self . n_hidden , self . n_layers , self . architecture ] # save attributes to file with open ( filename + \".pkl\" , 'wb' ) as f : pickle . dump ( attributes , f ) ten_to_predictions_np ( self , parameters_dict ) \u00b6 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from forward_pass_np Parameters: Name Type Description Default parameters_dict dict [numpy.ndarray] dictionary of (arrays of) parameters required Returns: Type Description numpy.ndarray 10^output predictions Source code in cosmopower/cosmopower_NN.py def ten_to_predictions_np ( self , parameters_dict ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ten_to_predictions_tf ( self , parameters_tensor ) \u00b6 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of predictions_tf Parameters: Name Type Description Default parameters_tensor Tensor input parameters required Returns: Type Description Tensor 10^output predictions Source code in cosmopower/cosmopower_NN.py @tf . function def ten_to_predictions_tf ( self , parameters_tensor ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_tensor )) train ( self , training_parameters , training_features , filename_saved_model , validation_split = 0.1 , learning_rates = [ 0.01 , 0.001 , 0.0001 , 1e-05 , 1e-06 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ]) \u00b6 Train the model Parameters: Name Type Description Default training_parameters dict [numpy.ndarray] input parameters required training_features numpy.ndarray true features for training required filename_saved_model str filename tag where model will be saved required validation_split float percentage of training data used for validation 0.1 learning_rates list [float] learning rates for each step of learning schedule [0.01, 0.001, 0.0001, 1e-05, 1e-06] batch_sizes list [int] batch sizes for each step of learning schedule [1024, 1024, 1024, 1024, 1024] gradient_accumulation_steps list [int] batches for gradient accumulations for each step of learning schedule [1, 1, 1, 1, 1] patience_values list [int] early stopping patience for each step of learning schedule [100, 100, 100, 100, 100] max_epochs list [int] maximum number of epochs for each step of learning schedule [1000, 1000, 1000, 1000, 1000] Source code in cosmopower/cosmopower_NN.py def train ( self , training_parameters , training_features , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: training_parameters (dict [numpy.ndarray]): input parameters training_features (numpy.ndarray): true features for training filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_NN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # from dict to array training_parameters = self . dict_to_ordered_arr_np ( training_parameters ) # parameters standardisation self . parameters_mean = np . mean ( training_parameters , axis = 0 ) self . parameters_std = np . std ( training_parameters , axis = 0 ) # features standardisation self . features_mean = np . mean ( training_features , axis = 0 ) self . features_std = np . std ( training_features , axis = 0 ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std , dtype = dtype , name = 'features_std' ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # casting training_parameters = tf . convert_to_tensor ( training_parameters , dtype = dtype ) training_features = tf . convert_to_tensor ( training_features , dtype = dtype ) # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_features [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , feats in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , feats ) else : loss = self . training_step_with_accumulated_gradients ( theta , feats , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_features [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) training_step ( self , training_parameters , training_features ) \u00b6 Minimize loss Parameters: Name Type Description Default training_parameters Tensor input parameters required training_features Tensor true features required Returns: Type Description loss (Tensor) mean squared difference Source code in cosmopower/cosmopower_NN.py def training_step ( self , training_parameters , training_features ): r \"\"\" Minimize loss Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_features ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss training_step_with_accumulated_gradients ( self , training_parameters , training_features , accumulation_steps = 10 ) \u00b6 Minimize loss, breaking calculation into accumulated gradients Parameters: Name Type Description Default training_parameters Tensor input parameters required training_features Tensor true features required accumulation_steps int number of accumulated gradients 10 Returns: Type Description accumulated_loss (Tensor) mean squared difference Source code in cosmopower/cosmopower_NN.py def training_step_with_accumulated_gradients ( self , training_parameters , training_features , accumulation_steps = 10 ): r \"\"\" Minimize loss, breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_features )) . batch ( int ( training_features . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_features_ , in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters_ , training_features_ ) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss update_emulator_parameters ( self ) \u00b6 Update emulator parameters before saving them Source code in cosmopower/cosmopower_NN.py def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put mean and std parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy ()","title":"cosmopower_NN"},{"location":"API/cosmopower_NN-reference/#cosmopower_nn-module","text":"","title":"cosmopower_NN module"},{"location":"API/cosmopower_NN-reference/#cosmopower.cosmopower_NN.cosmopower_NN","text":"Mapping between cosmological parameters and (log)-power spectra Attributes: Name Type Description parameters list [str] model parameters, sorted in the desired order modes numpy.ndarray multipoles or k-values in the (log)-spectra parameters_mean numpy.ndarray mean of input parameters parameters_std numpy.ndarray std of input parameters features_mean numpy.ndarray mean of output features features_std numpy.ndarray std of output features n_hidden list [int] number of nodes for each hidden layer restore bool whether to restore a previously trained model or not restore_filename str filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable bool training layers optimizer tf.keras.optimizers optimizer for training verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_NN.py class cosmopower_NN ( tf . keras . Model ): r \"\"\" Mapping between cosmological parameters and (log)-power spectra Attributes: parameters (list [str]): model parameters, sorted in the desired order modes (numpy.ndarray): multipoles or k-values in the (log)-spectra parameters_mean (numpy.ndarray): mean of input parameters parameters_std (numpy.ndarray): std of input parameters features_mean (numpy.ndarray): mean of output features features_std (numpy.ndarray): std of output features n_hidden (list [int]): number of nodes for each hidden layer restore (bool): whether to restore a previously trained model or not restore_filename (str): filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable (bool): training layers optimizer (tf.keras.optimizers): optimizer for training verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , parameters = None , modes = None , parameters_mean = None , parameters_std = None , features_mean = None , features_std = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): \"\"\" Constructor \"\"\" # super super ( cosmopower_NN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # attributes self . parameters = parameters self . n_parameters = len ( self . parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_modes ] self . n_layers = len ( self . architecture ) - 1 # input parameters mean and std self . parameters_mean_ = parameters_mean if parameters_mean is not None else np . zeros ( self . n_parameters ) self . parameters_std_ = parameters_std if parameters_std is not None else np . ones ( self . n_parameters ) # (log)-spectra mean and std self . features_mean_ = features_mean if features_mean is not None else np . zeros ( self . n_modes ) self . features_std_ = features_std if features_std is not None else np . ones ( self . n_modes ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , 1e-3 ), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) # optimizer self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_NN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_modes } output modes, \\n \" \\ f \"using { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) # ========== TENSORFLOW implementation =============== # non-linear activation function def activation ( self , x , alpha , beta ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) # tensor predictions @tf . function def predictions_tf ( self , parameters_tensor ): r \"\"\" Prediction given tensor of input parameters, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale -> output predictions return tf . add ( tf . multiply ( layers [ - 1 ], self . features_std ), self . features_mean ) # tensor 10.**predictions @tf . function def ten_to_predictions_tf ( self , parameters_tensor ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_tensor )) # ============= SAVE/LOAD model ============= # save network parameters to Numpy arrays def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put mean and std parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy () # save def save ( self , filename ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . features_mean_ , self . features_std_ , self . n_parameters , self . parameters , self . n_modes , self . modes , self . n_hidden , self . n_layers , self . architecture ] # save attributes to file with open ( filename + \".pkl\" , 'wb' ) as f : pickle . dump ( attributes , f ) # restore attributes def restore ( self , filename ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes with open ( filename + \".pkl\" , 'rb' ) as f : self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . n_parameters , self . parameters , \\ self . n_modes , self . modes , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) # ========== NUMPY implementation =============== # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # forward prediction given input parameters implemented in Numpy def forward_pass_np ( self , parameters_arr ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (standardised) predictions layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale and output return layers [ - 1 ] * self . features_std_ + self . features_mean_ # Numpy array predictions def predictions_np ( self , parameters_dict ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) # Numpy array 10.**predictions def ten_to_predictions_np ( self , parameters_dict ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ### Infrastructure for network training ### @tf . function def compute_loss ( self , training_parameters , training_features ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) @tf . function def compute_loss_and_gradients ( self , training_parameters , training_features ): r \"\"\" Computes mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . predictions_tf ( training_parameters ), training_features ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients def training_step ( self , training_parameters , training_features ): r \"\"\" Minimize loss Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_features ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss def training_step_with_accumulated_gradients ( self , training_parameters , training_features , accumulation_steps = 10 ): r \"\"\" Minimize loss, breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): input parameters training_features (Tensor): true features accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_features )) . batch ( int ( training_features . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_features_ , in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters_ , training_features_ ) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_features_ . shape [ 0 ] / training_features . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss # ========================================== # main TRAINING function # ========================================== def train ( self , training_parameters , training_features , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: training_parameters (dict [numpy.ndarray]): input parameters training_features (numpy.ndarray): true features for training filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_NN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # from dict to array training_parameters = self . dict_to_ordered_arr_np ( training_parameters ) # parameters standardisation self . parameters_mean = np . mean ( training_parameters , axis = 0 ) self . parameters_std = np . std ( training_parameters , axis = 0 ) # features standardisation self . features_mean = np . mean ( training_features , axis = 0 ) self . features_std = np . std ( training_features , axis = 0 ) # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std , dtype = dtype , name = 'parameters_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std , dtype = dtype , name = 'features_std' ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # casting training_parameters = tf . convert_to_tensor ( training_parameters , dtype = dtype ) training_features = tf . convert_to_tensor ( training_features , dtype = dtype ) # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_features [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , feats in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , feats ) else : loss = self . training_step_with_accumulated_gradients ( theta , feats , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_features [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' )","title":"cosmopower_NN"},{"location":"API/cosmopower_PCA-reference/","text":"cosmopower_PCA module \u00b6 cosmopower_PCA \u00b6 Principal Component Analysis of (log)-power spectra Attributes: Name Type Description parameters list model parameters, sorted in the desired order modes numpy.ndarray multipoles or k-values in the (log)-spectra n_pcas int number of PCA components parameters_filenames list [str] list of .npz filenames for parameters features_filenames list [str] list of .npz filenames for (log)-spectra verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_PCA.py class cosmopower_PCA (): r \"\"\" Principal Component Analysis of (log)-power spectra Attributes: parameters (list): model parameters, sorted in the desired order modes (numpy.ndarray): multipoles or k-values in the (log)-spectra n_pcas (int): number of PCA components parameters_filenames (list [str]): list of .npz filenames for parameters features_filenames (list [str]): list of .npz filenames for (log)-spectra verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , parameters , modes , n_pcas , parameters_filenames , features_filenames , verbose = False , ): r \"\"\" Constructor \"\"\" # attributes self . parameters = parameters self . n_parameters = len ( parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_pcas = n_pcas self . parameters_filenames = parameters_filenames self . features_filenames = features_filenames self . n_batches = len ( self . parameters_filenames ) # PCA object self . PCA = IncrementalPCA ( n_components = self . n_pcas ) # verbose self . verbose = verbose # print initialization info, if verbose if self . verbose : print ( f \" \\n Initialized cosmopower_PCA compression with { self . n_pcas } components \\n \" ) # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: input parameters sorted according to `parameters` \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # compute mean and std for (log)-spectra and parameters def standardise_features_and_parameters ( self ): r \"\"\" Compute mean and std for (log)-spectra and parameters \"\"\" # mean and std self . features_mean = np . zeros ( self . n_modes ) self . features_std = np . zeros ( self . n_modes ) self . parameters_mean = np . zeros ( self . n_parameters ) self . parameters_std = np . zeros ( self . n_parameters ) # loop over training data files, accumulate means and stds for i in range ( self . n_batches ): features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] parameters = self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + \".npz\" )) # accumulate self . features_mean += np . mean ( features , axis = 0 ) / self . n_batches self . features_std += np . std ( features , axis = 0 ) / self . n_batches self . parameters_mean += np . mean ( parameters , axis = 0 ) / self . n_batches self . parameters_std += np . std ( parameters , axis = 0 ) / self . n_batches # train PCA incrementally def train_pca ( self ): r \"\"\" Train PCA incrementally \"\"\" # loop over training data files, increment PCA with trange ( self . n_batches ) as t : for i in t : # load (log)-spectra and mean+std features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # partial PCA fit self . PCA . partial_fit ( normalised_features ) # set the PCA transform matrix self . pca_transform_matrix = self . PCA . components_ # transform the training data set to PCA basis def transform_and_stack_training_data ( self , filename = './tmp' , retain = True , ): r \"\"\" Transform the training data set to PCA basis Parameters: filename (str): filename tag (no suffix) for PCA coefficients and parameters retain (bool): whether to retain training data as attributes \"\"\" if self . verbose : print ( \"starting PCA compression\" ) self . standardise_features_and_parameters () self . train_pca () # transform the (log)-spectra to PCA basis training_pca = np . concatenate ([ self . PCA . transform (( np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] - self . features_mean ) / self . features_std ) for i in range ( self . n_batches )]) # stack the input parameters training_parameters = np . concatenate ([ self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + '.npz' )) for i in range ( self . n_batches )]) # mean and std of PCA basis self . pca_mean = np . mean ( training_pca , axis = 0 ) self . pca_std = np . std ( training_pca , axis = 0 ) # save stacked transformed training data self . pca_filename = filename np . save ( self . pca_filename + '_pca.npy' , training_pca ) np . save ( self . pca_filename + '_parameters.npy' , training_parameters ) # retain training data as attributes if retain == True if retain : self . training_pca = training_pca self . training_parameters = training_parameters if self . verbose : print ( \"PCA compression done\" ) if retain : print ( \"parameters and PCA coefficients of training set stored in memory\" ) # validate PCA given some validation data def validate_pca_basis ( self , features_filename , ): r \"\"\" Validate PCA given some validation data Parameters: features_filename (str): filename tag (no suffix) for validation (log)-spectra Returns: features_pca (numpy.ndarray): PCA of validation (log)-spectra features_in_basis (numpy.ndarray): inverse PCA transform of validation (log)-spectra \"\"\" # load (log)-spectra and standardise features = np . load ( features_filename + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # transform to PCA basis and back features_pca = self . PCA . transform ( normalised_features ) features_in_basis = np . dot ( features_pca , self . pca_transform_matrix ) * self . features_std + self . features_mean # return PCA coefficients and (log)-spectra in basis return features_pca , features_in_basis __init__ ( self , parameters , modes , n_pcas , parameters_filenames , features_filenames , verbose = False ) special \u00b6 Constructor Source code in cosmopower/cosmopower_PCA.py def __init__ ( self , parameters , modes , n_pcas , parameters_filenames , features_filenames , verbose = False , ): r \"\"\" Constructor \"\"\" # attributes self . parameters = parameters self . n_parameters = len ( parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_pcas = n_pcas self . parameters_filenames = parameters_filenames self . features_filenames = features_filenames self . n_batches = len ( self . parameters_filenames ) # PCA object self . PCA = IncrementalPCA ( n_components = self . n_pcas ) # verbose self . verbose = verbose # print initialization info, if verbose if self . verbose : print ( f \" \\n Initialized cosmopower_PCA compression with { self . n_pcas } components \\n \" ) dict_to_ordered_arr_np ( self , input_dict ) \u00b6 Sort input parameters Parameters: Name Type Description Default input_dict dict [numpy.ndarray] input dict of (arrays of) parameters to be sorted required Returns: Type Description numpy.ndarray input parameters sorted according to parameters Source code in cosmopower/cosmopower_PCA.py def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: input parameters sorted according to `parameters` \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) standardise_features_and_parameters ( self ) \u00b6 Compute mean and std for (log)-spectra and parameters Source code in cosmopower/cosmopower_PCA.py def standardise_features_and_parameters ( self ): r \"\"\" Compute mean and std for (log)-spectra and parameters \"\"\" # mean and std self . features_mean = np . zeros ( self . n_modes ) self . features_std = np . zeros ( self . n_modes ) self . parameters_mean = np . zeros ( self . n_parameters ) self . parameters_std = np . zeros ( self . n_parameters ) # loop over training data files, accumulate means and stds for i in range ( self . n_batches ): features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] parameters = self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + \".npz\" )) # accumulate self . features_mean += np . mean ( features , axis = 0 ) / self . n_batches self . features_std += np . std ( features , axis = 0 ) / self . n_batches self . parameters_mean += np . mean ( parameters , axis = 0 ) / self . n_batches self . parameters_std += np . std ( parameters , axis = 0 ) / self . n_batches train_pca ( self ) \u00b6 Train PCA incrementally Source code in cosmopower/cosmopower_PCA.py def train_pca ( self ): r \"\"\" Train PCA incrementally \"\"\" # loop over training data files, increment PCA with trange ( self . n_batches ) as t : for i in t : # load (log)-spectra and mean+std features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # partial PCA fit self . PCA . partial_fit ( normalised_features ) # set the PCA transform matrix self . pca_transform_matrix = self . PCA . components_ transform_and_stack_training_data ( self , filename = './tmp' , retain = True ) \u00b6 Transform the training data set to PCA basis Parameters: Name Type Description Default filename str filename tag (no suffix) for PCA coefficients and parameters './tmp' retain bool whether to retain training data as attributes True Source code in cosmopower/cosmopower_PCA.py def transform_and_stack_training_data ( self , filename = './tmp' , retain = True , ): r \"\"\" Transform the training data set to PCA basis Parameters: filename (str): filename tag (no suffix) for PCA coefficients and parameters retain (bool): whether to retain training data as attributes \"\"\" if self . verbose : print ( \"starting PCA compression\" ) self . standardise_features_and_parameters () self . train_pca () # transform the (log)-spectra to PCA basis training_pca = np . concatenate ([ self . PCA . transform (( np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] - self . features_mean ) / self . features_std ) for i in range ( self . n_batches )]) # stack the input parameters training_parameters = np . concatenate ([ self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + '.npz' )) for i in range ( self . n_batches )]) # mean and std of PCA basis self . pca_mean = np . mean ( training_pca , axis = 0 ) self . pca_std = np . std ( training_pca , axis = 0 ) # save stacked transformed training data self . pca_filename = filename np . save ( self . pca_filename + '_pca.npy' , training_pca ) np . save ( self . pca_filename + '_parameters.npy' , training_parameters ) # retain training data as attributes if retain == True if retain : self . training_pca = training_pca self . training_parameters = training_parameters if self . verbose : print ( \"PCA compression done\" ) if retain : print ( \"parameters and PCA coefficients of training set stored in memory\" ) validate_pca_basis ( self , features_filename ) \u00b6 Validate PCA given some validation data Parameters: Name Type Description Default features_filename str filename tag (no suffix) for validation (log)-spectra required Returns: Type Description features_pca (numpy.ndarray) PCA of validation (log)-spectra features_in_basis (numpy.ndarray): inverse PCA transform of validation (log)-spectra Source code in cosmopower/cosmopower_PCA.py def validate_pca_basis ( self , features_filename , ): r \"\"\" Validate PCA given some validation data Parameters: features_filename (str): filename tag (no suffix) for validation (log)-spectra Returns: features_pca (numpy.ndarray): PCA of validation (log)-spectra features_in_basis (numpy.ndarray): inverse PCA transform of validation (log)-spectra \"\"\" # load (log)-spectra and standardise features = np . load ( features_filename + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # transform to PCA basis and back features_pca = self . PCA . transform ( normalised_features ) features_in_basis = np . dot ( features_pca , self . pca_transform_matrix ) * self . features_std + self . features_mean # return PCA coefficients and (log)-spectra in basis return features_pca , features_in_basis","title":"cosmopower_PCA"},{"location":"API/cosmopower_PCA-reference/#cosmopower_pca-module","text":"","title":"cosmopower_PCA module"},{"location":"API/cosmopower_PCA-reference/#cosmopower.cosmopower_PCA.cosmopower_PCA","text":"Principal Component Analysis of (log)-power spectra Attributes: Name Type Description parameters list model parameters, sorted in the desired order modes numpy.ndarray multipoles or k-values in the (log)-spectra n_pcas int number of PCA components parameters_filenames list [str] list of .npz filenames for parameters features_filenames list [str] list of .npz filenames for (log)-spectra verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_PCA.py class cosmopower_PCA (): r \"\"\" Principal Component Analysis of (log)-power spectra Attributes: parameters (list): model parameters, sorted in the desired order modes (numpy.ndarray): multipoles or k-values in the (log)-spectra n_pcas (int): number of PCA components parameters_filenames (list [str]): list of .npz filenames for parameters features_filenames (list [str]): list of .npz filenames for (log)-spectra verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , parameters , modes , n_pcas , parameters_filenames , features_filenames , verbose = False , ): r \"\"\" Constructor \"\"\" # attributes self . parameters = parameters self . n_parameters = len ( parameters ) self . modes = modes self . n_modes = len ( self . modes ) self . n_pcas = n_pcas self . parameters_filenames = parameters_filenames self . features_filenames = features_filenames self . n_batches = len ( self . parameters_filenames ) # PCA object self . PCA = IncrementalPCA ( n_components = self . n_pcas ) # verbose self . verbose = verbose # print initialization info, if verbose if self . verbose : print ( f \" \\n Initialized cosmopower_PCA compression with { self . n_pcas } components \\n \" ) # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: input parameters sorted according to `parameters` \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # compute mean and std for (log)-spectra and parameters def standardise_features_and_parameters ( self ): r \"\"\" Compute mean and std for (log)-spectra and parameters \"\"\" # mean and std self . features_mean = np . zeros ( self . n_modes ) self . features_std = np . zeros ( self . n_modes ) self . parameters_mean = np . zeros ( self . n_parameters ) self . parameters_std = np . zeros ( self . n_parameters ) # loop over training data files, accumulate means and stds for i in range ( self . n_batches ): features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] parameters = self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + \".npz\" )) # accumulate self . features_mean += np . mean ( features , axis = 0 ) / self . n_batches self . features_std += np . std ( features , axis = 0 ) / self . n_batches self . parameters_mean += np . mean ( parameters , axis = 0 ) / self . n_batches self . parameters_std += np . std ( parameters , axis = 0 ) / self . n_batches # train PCA incrementally def train_pca ( self ): r \"\"\" Train PCA incrementally \"\"\" # loop over training data files, increment PCA with trange ( self . n_batches ) as t : for i in t : # load (log)-spectra and mean+std features = np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # partial PCA fit self . PCA . partial_fit ( normalised_features ) # set the PCA transform matrix self . pca_transform_matrix = self . PCA . components_ # transform the training data set to PCA basis def transform_and_stack_training_data ( self , filename = './tmp' , retain = True , ): r \"\"\" Transform the training data set to PCA basis Parameters: filename (str): filename tag (no suffix) for PCA coefficients and parameters retain (bool): whether to retain training data as attributes \"\"\" if self . verbose : print ( \"starting PCA compression\" ) self . standardise_features_and_parameters () self . train_pca () # transform the (log)-spectra to PCA basis training_pca = np . concatenate ([ self . PCA . transform (( np . load ( self . features_filenames [ i ] + \".npz\" )[ 'features' ] - self . features_mean ) / self . features_std ) for i in range ( self . n_batches )]) # stack the input parameters training_parameters = np . concatenate ([ self . dict_to_ordered_arr_np ( np . load ( self . parameters_filenames [ i ] + '.npz' )) for i in range ( self . n_batches )]) # mean and std of PCA basis self . pca_mean = np . mean ( training_pca , axis = 0 ) self . pca_std = np . std ( training_pca , axis = 0 ) # save stacked transformed training data self . pca_filename = filename np . save ( self . pca_filename + '_pca.npy' , training_pca ) np . save ( self . pca_filename + '_parameters.npy' , training_parameters ) # retain training data as attributes if retain == True if retain : self . training_pca = training_pca self . training_parameters = training_parameters if self . verbose : print ( \"PCA compression done\" ) if retain : print ( \"parameters and PCA coefficients of training set stored in memory\" ) # validate PCA given some validation data def validate_pca_basis ( self , features_filename , ): r \"\"\" Validate PCA given some validation data Parameters: features_filename (str): filename tag (no suffix) for validation (log)-spectra Returns: features_pca (numpy.ndarray): PCA of validation (log)-spectra features_in_basis (numpy.ndarray): inverse PCA transform of validation (log)-spectra \"\"\" # load (log)-spectra and standardise features = np . load ( features_filename + \".npz\" )[ 'features' ] normalised_features = ( features - self . features_mean ) / self . features_std # transform to PCA basis and back features_pca = self . PCA . transform ( normalised_features ) features_in_basis = np . dot ( features_pca , self . pca_transform_matrix ) * self . features_std + self . features_mean # return PCA coefficients and (log)-spectra in basis return features_pca , features_in_basis","title":"cosmopower_PCA"},{"location":"API/cosmopower_PCAplusNN-reference/","text":"cosmopower_PCAplusNN module \u00b6 cosmopower_PCAplusNN ( Model ) \u00b6 Mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Requires PCA compression to have been previously computed ( cosmopower_PCA ). Attributes: Name Type Description cp_pca cosmopower_PCA cosmopower_PCA instance n_hidden list [int] list with number of nodes for each hidden layer restore bool whether to restore a previously trained model or not restore_filename str filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable bool training layers optimizer tf.keras.optimizer optimizer for training verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_PCAplusNN.py class cosmopower_PCAplusNN ( tf . keras . Model ): r \"\"\" Mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Requires PCA compression to have been previously computed (`cosmopower_PCA`). Attributes: cp_pca (cosmopower_PCA): `cosmopower_PCA` instance n_hidden (list [int]): list with number of nodes for each hidden layer restore (bool): whether to restore a previously trained model or not restore_filename (str): filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable (bool): training layers optimizer (tf.keras.optimizer): optimizer for training verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , cp_pca = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): r \"\"\" Constructor. \"\"\" # super super ( cosmopower_PCAplusNN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # PCA compression, previously computed self . cp_pca = cp_pca # parameters self . parameters = self . cp_pca . parameters self . n_parameters = len ( self . parameters ) self . pca_transform_matrix_ = self . cp_pca . pca_transform_matrix self . modes = self . cp_pca . modes self . n_modes = self . cp_pca . n_modes self . n_pcas = self . pca_transform_matrix_ . shape [ 0 ] self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_pcas ] self . n_layers = len ( self . architecture ) - 1 # standardisation # input parameters mean and std self . parameters_mean_ = self . cp_pca . parameters_mean self . parameters_std_ = self . cp_pca . parameters_std # PCA mean and std self . pca_mean_ = self . cp_pca . pca_mean self . pca_std_ = self . cp_pca . pca_std # spectra mean and std self . features_mean_ = self . cp_pca . features_mean self . features_std_ = self . cp_pca . features_std # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # PCA mean and std self . pca_mean = tf . constant ( self . pca_mean_ , dtype = dtype , name = 'pca_mean' ) self . pca_std = tf . constant ( self . pca_std_ , dtype = dtype , name = 'pca_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # pca transform matrix self . pca_transform_matrix = tf . constant ( self . pca_transform_matrix_ , dtype = dtype , name = 'pca_transform_matrix' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , np . sqrt ( 2. / self . n_parameters )), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_PCAplusNN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_pcas } PCA components \\n \" \\ f \"and then inverting the PCA compression to obtain { self . n_modes } modes \\n \" \\ f \"The model uses { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) # ========== TENSORFLOW implementation =============== # non-linear activation function def activation ( self , x , alpha , beta , ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) # forward pass through the network to predict PCA coefficients def forward_pass_tf ( self , parameters_tensor , ): r \"\"\" Forward pass through the network to predict the PCA coefficients, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): tensor of shape (number_of_cosmologies, number_of_cosmological_parameters) Returns: Tensor: PCA predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale the output (predicted PCA coefficients) and return return tf . add ( tf . multiply ( layers [ - 1 ], self . pca_std ), self . pca_mean ) # pass inputs through the network to predict (log)-spectrum @tf . function def predictions_tf ( self , parameters_tensor , ): r \"\"\" Predictions given tensor of input parameters, fully implemented in TensorFlow. Calls ``forward_pass_tf`` and inverts PCA Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" # pass through network to compute PCA coefficients pca_coefficients = self . forward_pass_tf ( parameters_tensor ) # transform from PCA to normalized spectrum basis; shift and re-scale normalised (log)-spectrum -> (log)-spectrum return tf . add ( tf . multiply ( tf . matmul ( pca_coefficients , self . pca_transform_matrix ), self . features_std ), self . features_mean ) # tensor 10.**predictions @tf . function def ten_to_predictions_tf ( self , parameters_dict , ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_dict )) # save network parameters to Numpy arrays def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put shift and scale parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . pca_mean_ = self . pca_mean . numpy () self . pca_std_ = self . pca_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy () # pca transform matrix self . pca_transform_matrix_ = self . pca_transform_matrix . numpy () # save def save ( self , filename , ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . pca_mean_ , self . pca_std_ , self . features_mean_ , self . features_std_ , self . parameters , self . n_parameters , self . modes , self . n_modes , self . n_pcas , self . pca_transform_matrix_ , self . n_hidden , self . n_layers , self . architecture , ] # save attributes to file f = open ( filename + \".pkl\" , 'wb' ) pickle . dump ( attributes , f ) f . close () # restore attributes def restore ( self , filename , ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes f = open ( filename + \".pkl\" , 'rb' ) self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . pca_mean_ , self . pca_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . parameters , self . n_parameters , \\ self . modes , self . n_modes , \\ self . n_pcas , self . pca_transform_matrix_ , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) f . close () # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # forward prediction given input parameters implemented in Numpy def forward_pass_np ( self , parameters_arr , ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (normalized) PCA coefficients layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale PCA coefficients, multiply out PCA basis -> normalised (log)-spectrum, shift and re-scale (log)-spectrum -> output (log)-spectrum return np . dot ( layers [ - 1 ] * self . pca_std_ + self . pca_mean_ , self . pca_transform_matrix_ ) * self . features_std_ + self . features_mean_ def predictions_np ( self , parameters_dict , ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) # 10.**predictions def ten_to_predictions_np ( self , parameters_dict , ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ### Infrastructure for network training ### @tf . function def compute_loss ( self , training_parameters , training_pca , ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) @tf . function def compute_loss_and_gradients ( self , training_parameters , training_pca , ): r \"\"\" Compute mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients def training_step ( self , training_parameters , training_pca , ): r \"\"\" Optimizes loss Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_pca ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss def training_step_with_accumulated_gradients ( self , training_parameters , training_pca , accumulation_steps = 10 , ): r \"\"\" Optimize loss breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): tensor of input parameters for the network training_pca (Tensor): tensor of true PCA components accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_pca )) . batch ( int ( training_pca . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_pca_ in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients_pca ( training_parameters_ , training_pca_ ,) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss # ========================================== # main TRAINING function # ========================================== def train ( self , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_PCAplusNN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # casting training_parameters = tf . convert_to_tensor ( self . cp_pca . training_parameters , dtype = dtype ) training_pca = tf . convert_to_tensor ( self . cp_pca . training_pca , dtype = dtype ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_pca [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , pca in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , pca ) else : loss = self . training_step_with_accumulated_gradients ( theta , pca , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_pca [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) __init__ ( self , cp_pca = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer =< keras . optimizer_v2 . adam . Adam object at 0x7f9fb895ad10 > , verbose = False ) special \u00b6 Constructor. Source code in cosmopower/cosmopower_PCAplusNN.py def __init__ ( self , cp_pca = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): r \"\"\" Constructor. \"\"\" # super super ( cosmopower_PCAplusNN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # PCA compression, previously computed self . cp_pca = cp_pca # parameters self . parameters = self . cp_pca . parameters self . n_parameters = len ( self . parameters ) self . pca_transform_matrix_ = self . cp_pca . pca_transform_matrix self . modes = self . cp_pca . modes self . n_modes = self . cp_pca . n_modes self . n_pcas = self . pca_transform_matrix_ . shape [ 0 ] self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_pcas ] self . n_layers = len ( self . architecture ) - 1 # standardisation # input parameters mean and std self . parameters_mean_ = self . cp_pca . parameters_mean self . parameters_std_ = self . cp_pca . parameters_std # PCA mean and std self . pca_mean_ = self . cp_pca . pca_mean self . pca_std_ = self . cp_pca . pca_std # spectra mean and std self . features_mean_ = self . cp_pca . features_mean self . features_std_ = self . cp_pca . features_std # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # PCA mean and std self . pca_mean = tf . constant ( self . pca_mean_ , dtype = dtype , name = 'pca_mean' ) self . pca_std = tf . constant ( self . pca_std_ , dtype = dtype , name = 'pca_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # pca transform matrix self . pca_transform_matrix = tf . constant ( self . pca_transform_matrix_ , dtype = dtype , name = 'pca_transform_matrix' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , np . sqrt ( 2. / self . n_parameters )), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_PCAplusNN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_pcas } PCA components \\n \" \\ f \"and then inverting the PCA compression to obtain { self . n_modes } modes \\n \" \\ f \"The model uses { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) activation ( self , x , alpha , beta ) \u00b6 Non-linear activation function Parameters: Name Type Description Default x Tensor linear output from previous layer required alpha Tensor trainable parameter required beta Tensor trainable parameter required Returns: Type Description Tensor the result of applying the non-linear activation function to the linear output of the layer Source code in cosmopower/cosmopower_PCAplusNN.py def activation ( self , x , alpha , beta , ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) compute_loss ( self , training_parameters , training_pca ) \u00b6 Mean squared difference Parameters: Name Type Description Default training_parameters Tensor input parameters required training_pca Tensor true PCA components required Returns: Type Description Tensor mean squared difference Source code in cosmopower/cosmopower_PCAplusNN.py @tf . function def compute_loss ( self , training_parameters , training_pca , ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) compute_loss_and_gradients ( self , training_parameters , training_pca ) \u00b6 Compute mean squared difference and gradients Parameters: Name Type Description Default training_parameters Tensor input parameters required training_pca Tensor true PCA components required Returns: Type Description loss (Tensor) mean squared difference gradients (Tensor): gradients Source code in cosmopower/cosmopower_PCAplusNN.py @tf . function def compute_loss_and_gradients ( self , training_parameters , training_pca , ): r \"\"\" Compute mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients dict_to_ordered_arr_np ( self , input_dict ) \u00b6 Sort input parameters Parameters: Name Type Description Default input_dict dict [numpy.ndarray] input dict of (arrays of) parameters to be sorted required Returns: Type Description numpy.ndarray parameters sorted according to desired order Source code in cosmopower/cosmopower_PCAplusNN.py def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) forward_pass_np ( self , parameters_arr ) \u00b6 Forward pass through the network to predict the output, fully implemented in Numpy Parameters: Name Type Description Default parameters_arr numpy.ndarray array of input parameters required Returns: Type Description numpy.ndarray output predictions Source code in cosmopower/cosmopower_PCAplusNN.py def forward_pass_np ( self , parameters_arr , ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (normalized) PCA coefficients layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale PCA coefficients, multiply out PCA basis -> normalised (log)-spectrum, shift and re-scale (log)-spectrum -> output (log)-spectrum return np . dot ( layers [ - 1 ] * self . pca_std_ + self . pca_mean_ , self . pca_transform_matrix_ ) * self . features_std_ + self . features_mean_ forward_pass_tf ( self , parameters_tensor ) \u00b6 Forward pass through the network to predict the PCA coefficients, fully implemented in TensorFlow Parameters: Name Type Description Default parameters_tensor Tensor tensor of shape (number_of_cosmologies, number_of_cosmological_parameters) required Returns: Type Description Tensor PCA predictions Source code in cosmopower/cosmopower_PCAplusNN.py def forward_pass_tf ( self , parameters_tensor , ): r \"\"\" Forward pass through the network to predict the PCA coefficients, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): tensor of shape (number_of_cosmologies, number_of_cosmological_parameters) Returns: Tensor: PCA predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale the output (predicted PCA coefficients) and return return tf . add ( tf . multiply ( layers [ - 1 ], self . pca_std ), self . pca_mean ) predictions_np ( self , parameters_dict ) \u00b6 Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls forward_pass_np after ordering the input parameter dict Parameters: Name Type Description Default parameters_dict dict [numpy.ndarray] dictionary of (arrays of) parameters required Returns: Type Description numpy.ndarray output predictions Source code in cosmopower/cosmopower_PCAplusNN.py def predictions_np ( self , parameters_dict , ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) predictions_tf ( self , parameters_tensor ) \u00b6 Predictions given tensor of input parameters, fully implemented in TensorFlow. Calls forward_pass_tf and inverts PCA Parameters: Name Type Description Default parameters_tensor Tensor input parameters required Returns: Type Description Tensor output predictions Source code in cosmopower/cosmopower_PCAplusNN.py @tf . function def predictions_tf ( self , parameters_tensor , ): r \"\"\" Predictions given tensor of input parameters, fully implemented in TensorFlow. Calls ``forward_pass_tf`` and inverts PCA Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" # pass through network to compute PCA coefficients pca_coefficients = self . forward_pass_tf ( parameters_tensor ) # transform from PCA to normalized spectrum basis; shift and re-scale normalised (log)-spectrum -> (log)-spectrum return tf . add ( tf . multiply ( tf . matmul ( pca_coefficients , self . pca_transform_matrix ), self . features_std ), self . features_mean ) restore ( self , filename ) \u00b6 Load pre-trained model Parameters: Name Type Description Default filename str filename tag (without suffix) where model was saved required Source code in cosmopower/cosmopower_PCAplusNN.py def restore ( self , filename , ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes f = open ( filename + \".pkl\" , 'rb' ) self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . pca_mean_ , self . pca_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . parameters , self . n_parameters , \\ self . modes , self . n_modes , \\ self . n_pcas , self . pca_transform_matrix_ , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) f . close () save ( self , filename ) \u00b6 Save network parameters Parameters: Name Type Description Default filename str filename tag (without suffix) where model will be saved required Source code in cosmopower/cosmopower_PCAplusNN.py def save ( self , filename , ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . pca_mean_ , self . pca_std_ , self . features_mean_ , self . features_std_ , self . parameters , self . n_parameters , self . modes , self . n_modes , self . n_pcas , self . pca_transform_matrix_ , self . n_hidden , self . n_layers , self . architecture , ] # save attributes to file f = open ( filename + \".pkl\" , 'wb' ) pickle . dump ( attributes , f ) f . close () ten_to_predictions_np ( self , parameters_dict ) \u00b6 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from forward_pass_np Parameters: Name Type Description Default parameters_dict dict [numpy.ndarray] dictionary of (arrays of) parameters required Returns: Type Description numpy.ndarray 10^output predictions Source code in cosmopower/cosmopower_PCAplusNN.py def ten_to_predictions_np ( self , parameters_dict , ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ten_to_predictions_tf ( self , parameters_dict ) \u00b6 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of predictions_tf Parameters: Name Type Description Default parameters Tensor input parameters required Returns: Type Description Tensor 10^output predictions Source code in cosmopower/cosmopower_PCAplusNN.py @tf . function def ten_to_predictions_tf ( self , parameters_dict , ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_dict )) train ( self , filename_saved_model , validation_split = 0.1 , learning_rates = [ 0.01 , 0.001 , 0.0001 , 1e-05 , 1e-06 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ]) \u00b6 Train the model Parameters: Name Type Description Default filename_saved_model str filename tag where model will be saved required validation_split float percentage of training data used for validation 0.1 learning_rates list [float] learning rates for each step of learning schedule [0.01, 0.001, 0.0001, 1e-05, 1e-06] batch_sizes list [int] batch sizes for each step of learning schedule [1024, 1024, 1024, 1024, 1024] gradient_accumulation_steps list [int] batches for gradient accumulations for each step of learning schedule [1, 1, 1, 1, 1] patience_values list [int] early stopping patience for each step of learning schedule [100, 100, 100, 100, 100] max_epochs list [int] maximum number of epochs for each step of learning schedule [1000, 1000, 1000, 1000, 1000] Source code in cosmopower/cosmopower_PCAplusNN.py def train ( self , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_PCAplusNN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # casting training_parameters = tf . convert_to_tensor ( self . cp_pca . training_parameters , dtype = dtype ) training_pca = tf . convert_to_tensor ( self . cp_pca . training_pca , dtype = dtype ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_pca [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , pca in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , pca ) else : loss = self . training_step_with_accumulated_gradients ( theta , pca , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_pca [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) training_step ( self , training_parameters , training_pca ) \u00b6 Optimizes loss Parameters: Name Type Description Default training_parameters Tensor input parameters required training_pca Tensor true PCA components required Returns: Type Description loss (Tensor) mean squared difference Source code in cosmopower/cosmopower_PCAplusNN.py def training_step ( self , training_parameters , training_pca , ): r \"\"\" Optimizes loss Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_pca ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss training_step_with_accumulated_gradients ( self , training_parameters , training_pca , accumulation_steps = 10 ) \u00b6 Optimize loss breaking calculation into accumulated gradients Parameters: Name Type Description Default training_parameters Tensor tensor of input parameters for the network required training_pca Tensor tensor of true PCA components required accumulation_steps int number of accumulated gradients 10 Returns: Type Description accumulated_loss (Tensor) mean squared difference Source code in cosmopower/cosmopower_PCAplusNN.py def training_step_with_accumulated_gradients ( self , training_parameters , training_pca , accumulation_steps = 10 , ): r \"\"\" Optimize loss breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): tensor of input parameters for the network training_pca (Tensor): tensor of true PCA components accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_pca )) . batch ( int ( training_pca . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_pca_ in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients_pca ( training_parameters_ , training_pca_ ,) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss update_emulator_parameters ( self ) \u00b6 Update emulator parameters before saving them Source code in cosmopower/cosmopower_PCAplusNN.py def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put shift and scale parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . pca_mean_ = self . pca_mean . numpy () self . pca_std_ = self . pca_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy () # pca transform matrix self . pca_transform_matrix_ = self . pca_transform_matrix . numpy ()","title":"cosmopower_PCAplusNN"},{"location":"API/cosmopower_PCAplusNN-reference/#cosmopower_pcaplusnn-module","text":"","title":"cosmopower_PCAplusNN module"},{"location":"API/cosmopower_PCAplusNN-reference/#cosmopower.cosmopower_PCAplusNN.cosmopower_PCAplusNN","text":"Mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Requires PCA compression to have been previously computed ( cosmopower_PCA ). Attributes: Name Type Description cp_pca cosmopower_PCA cosmopower_PCA instance n_hidden list [int] list with number of nodes for each hidden layer restore bool whether to restore a previously trained model or not restore_filename str filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable bool training layers optimizer tf.keras.optimizer optimizer for training verbose bool whether to print messages at intermediate steps or not Source code in cosmopower/cosmopower_PCAplusNN.py class cosmopower_PCAplusNN ( tf . keras . Model ): r \"\"\" Mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Requires PCA compression to have been previously computed (`cosmopower_PCA`). Attributes: cp_pca (cosmopower_PCA): `cosmopower_PCA` instance n_hidden (list [int]): list with number of nodes for each hidden layer restore (bool): whether to restore a previously trained model or not restore_filename (str): filename tag (without suffix) for restoring trained model from file (this will be a pickle file with all of the model attributes and weights) trainable (bool): training layers optimizer (tf.keras.optimizer): optimizer for training verbose (bool): whether to print messages at intermediate steps or not \"\"\" def __init__ ( self , cp_pca = None , n_hidden = [ 512 , 512 , 512 ], restore = False , restore_filename = None , trainable = True , optimizer = tf . keras . optimizers . Adam (), verbose = False , ): r \"\"\" Constructor. \"\"\" # super super ( cosmopower_PCAplusNN , self ) . __init__ () # restore if restore is True : self . restore ( restore_filename ) # else set variables from input arguments else : # PCA compression, previously computed self . cp_pca = cp_pca # parameters self . parameters = self . cp_pca . parameters self . n_parameters = len ( self . parameters ) self . pca_transform_matrix_ = self . cp_pca . pca_transform_matrix self . modes = self . cp_pca . modes self . n_modes = self . cp_pca . n_modes self . n_pcas = self . pca_transform_matrix_ . shape [ 0 ] self . n_hidden = n_hidden # architecture self . architecture = [ self . n_parameters ] + self . n_hidden + [ self . n_pcas ] self . n_layers = len ( self . architecture ) - 1 # standardisation # input parameters mean and std self . parameters_mean_ = self . cp_pca . parameters_mean self . parameters_std_ = self . cp_pca . parameters_std # PCA mean and std self . pca_mean_ = self . cp_pca . pca_mean self . pca_std_ = self . cp_pca . pca_std # spectra mean and std self . features_mean_ = self . cp_pca . features_mean self . features_std_ = self . cp_pca . features_std # input parameters mean and std self . parameters_mean = tf . constant ( self . parameters_mean_ , dtype = dtype , name = 'parameters_mean' ) self . parameters_std = tf . constant ( self . parameters_std_ , dtype = dtype , name = 'parameters_std' ) # PCA mean and std self . pca_mean = tf . constant ( self . pca_mean_ , dtype = dtype , name = 'pca_mean' ) self . pca_std = tf . constant ( self . pca_std_ , dtype = dtype , name = 'pca_std' ) # (log)-spectra mean and std self . features_mean = tf . constant ( self . features_mean_ , dtype = dtype , name = 'features_mean' ) self . features_std = tf . constant ( self . features_std_ , dtype = dtype , name = 'features_std' ) # pca transform matrix self . pca_transform_matrix = tf . constant ( self . pca_transform_matrix_ , dtype = dtype , name = 'pca_transform_matrix' ) # weights, biases and activation function parameters for each layer of the network self . W = [] self . b = [] self . alphas = [] self . betas = [] for i in range ( self . n_layers ): self . W . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i ], self . architecture [ i + 1 ]], 0. , np . sqrt ( 2. / self . n_parameters )), name = \"W_\" + str ( i ), trainable = trainable )) self . b . append ( tf . Variable ( tf . zeros ([ self . architecture [ i + 1 ]]), name = \"b_\" + str ( i ), trainable = trainable )) for i in range ( self . n_layers - 1 ): self . alphas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"alphas_\" + str ( i ), trainable = trainable )) self . betas . append ( tf . Variable ( tf . random . normal ([ self . architecture [ i + 1 ]]), name = \"betas_\" + str ( i ), trainable = trainable )) # restore weights if restore = True if restore is True : for i in range ( self . n_layers ): self . W [ i ] . assign ( self . W_ [ i ]) self . b [ i ] . assign ( self . b_ [ i ]) for i in range ( self . n_layers - 1 ): self . alphas [ i ] . assign ( self . alphas_ [ i ]) self . betas [ i ] . assign ( self . betas_ [ i ]) self . optimizer = optimizer self . verbose = verbose # print initialization info, if verbose if self . verbose : multiline_str = \" \\n Initialized cosmopower_PCAplusNN model, \\n \" \\ f \"mapping { self . n_parameters } input parameters to { self . n_pcas } PCA components \\n \" \\ f \"and then inverting the PCA compression to obtain { self . n_modes } modes \\n \" \\ f \"The model uses { len ( self . n_hidden ) } hidden layers, \\n \" \\ f \"with { list ( self . n_hidden ) } nodes, respectively. \\n \" print ( multiline_str ) # ========== TENSORFLOW implementation =============== # non-linear activation function def activation ( self , x , alpha , beta , ): r \"\"\" Non-linear activation function Parameters: x (Tensor): linear output from previous layer alpha (Tensor): trainable parameter beta (Tensor): trainable parameter Returns: Tensor: the result of applying the non-linear activation function to the linear output of the layer \"\"\" return tf . multiply ( tf . add ( beta , tf . multiply ( tf . sigmoid ( tf . multiply ( alpha , x )), tf . subtract ( 1.0 , beta )) ), x ) # forward pass through the network to predict PCA coefficients def forward_pass_tf ( self , parameters_tensor , ): r \"\"\" Forward pass through the network to predict the PCA coefficients, fully implemented in TensorFlow Parameters: parameters_tensor (Tensor): tensor of shape (number_of_cosmologies, number_of_cosmological_parameters) Returns: Tensor: PCA predictions \"\"\" outputs = [] layers = [ tf . divide ( tf . subtract ( parameters_tensor , self . parameters_mean ), self . parameters_std )] for i in range ( self . n_layers - 1 ): # linear network operation outputs . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ i ]), self . b [ i ])) # non-linear activation function layers . append ( self . activation ( outputs [ - 1 ], self . alphas [ i ], self . betas [ i ])) # linear output layer layers . append ( tf . add ( tf . matmul ( layers [ - 1 ], self . W [ - 1 ]), self . b [ - 1 ])) # rescale the output (predicted PCA coefficients) and return return tf . add ( tf . multiply ( layers [ - 1 ], self . pca_std ), self . pca_mean ) # pass inputs through the network to predict (log)-spectrum @tf . function def predictions_tf ( self , parameters_tensor , ): r \"\"\" Predictions given tensor of input parameters, fully implemented in TensorFlow. Calls ``forward_pass_tf`` and inverts PCA Parameters: parameters_tensor (Tensor): input parameters Returns: Tensor: output predictions \"\"\" # pass through network to compute PCA coefficients pca_coefficients = self . forward_pass_tf ( parameters_tensor ) # transform from PCA to normalized spectrum basis; shift and re-scale normalised (log)-spectrum -> (log)-spectrum return tf . add ( tf . multiply ( tf . matmul ( pca_coefficients , self . pca_transform_matrix ), self . features_std ), self . features_mean ) # tensor 10.**predictions @tf . function def ten_to_predictions_tf ( self , parameters_dict , ): r \"\"\" 10^predictions given tensor of input parameters, fully implemented in TensorFlow. It raises 10 to the output of ``predictions_tf`` Parameters: parameters (Tensor): input parameters Returns: Tensor: 10^output predictions \"\"\" return tf . pow ( 10. , self . predictions_tf ( parameters_dict )) # save network parameters to Numpy arrays def update_emulator_parameters ( self ): r \"\"\" Update emulator parameters before saving them \"\"\" # put network parameters to numpy arrays self . W_ = [ self . W [ i ] . numpy () for i in range ( self . n_layers )] self . b_ = [ self . b [ i ] . numpy () for i in range ( self . n_layers )] self . alphas_ = [ self . alphas [ i ] . numpy () for i in range ( self . n_layers - 1 )] self . betas_ = [ self . betas [ i ] . numpy () for i in range ( self . n_layers - 1 )] # put shift and scale parameters to numpy arrays self . parameters_mean_ = self . parameters_mean . numpy () self . parameters_std_ = self . parameters_std . numpy () self . pca_mean_ = self . pca_mean . numpy () self . pca_std_ = self . pca_std . numpy () self . features_mean_ = self . features_mean . numpy () self . features_std_ = self . features_std . numpy () # pca transform matrix self . pca_transform_matrix_ = self . pca_transform_matrix . numpy () # save def save ( self , filename , ): r \"\"\" Save network parameters Parameters: filename (str): filename tag (without suffix) where model will be saved \"\"\" # attributes attributes = [ self . W_ , self . b_ , self . alphas_ , self . betas_ , self . parameters_mean_ , self . parameters_std_ , self . pca_mean_ , self . pca_std_ , self . features_mean_ , self . features_std_ , self . parameters , self . n_parameters , self . modes , self . n_modes , self . n_pcas , self . pca_transform_matrix_ , self . n_hidden , self . n_layers , self . architecture , ] # save attributes to file f = open ( filename + \".pkl\" , 'wb' ) pickle . dump ( attributes , f ) f . close () # restore attributes def restore ( self , filename , ): r \"\"\" Load pre-trained model Parameters: filename (str): filename tag (without suffix) where model was saved \"\"\" # load attributes f = open ( filename + \".pkl\" , 'rb' ) self . W_ , self . b_ , self . alphas_ , self . betas_ , \\ self . parameters_mean_ , self . parameters_std_ , \\ self . pca_mean_ , self . pca_std_ , \\ self . features_mean_ , self . features_std_ , \\ self . parameters , self . n_parameters , \\ self . modes , self . n_modes , \\ self . n_pcas , self . pca_transform_matrix_ , \\ self . n_hidden , self . n_layers , self . architecture = pickle . load ( f ) f . close () # auxiliary function to sort input parameters def dict_to_ordered_arr_np ( self , input_dict , ): r \"\"\" Sort input parameters Parameters: input_dict (dict [numpy.ndarray]): input dict of (arrays of) parameters to be sorted Returns: numpy.ndarray: parameters sorted according to desired order \"\"\" if self . parameters is not None : return np . stack ([ input_dict [ k ] for k in self . parameters ], axis = 1 ) else : return np . stack ([ input_dict [ k ] for k in input_dict ], axis = 1 ) # forward prediction given input parameters implemented in Numpy def forward_pass_np ( self , parameters_arr , ): r \"\"\" Forward pass through the network to predict the output, fully implemented in Numpy Parameters: parameters_arr (numpy.ndarray): array of input parameters Returns: numpy.ndarray: output predictions \"\"\" # forward pass through the network act = [] layers = [( parameters_arr - self . parameters_mean_ ) / self . parameters_std_ ] for i in range ( self . n_layers - 1 ): # linear network operation act . append ( np . dot ( layers [ - 1 ], self . W_ [ i ]) + self . b_ [ i ]) # pass through activation function layers . append (( self . betas_ [ i ] + ( 1. - self . betas_ [ i ]) * 1. / ( 1. + np . exp ( - self . alphas_ [ i ] * act [ - 1 ]))) * act [ - 1 ]) # final (linear) layer -> (normalized) PCA coefficients layers . append ( np . dot ( layers [ - 1 ], self . W_ [ - 1 ]) + self . b_ [ - 1 ]) # rescale PCA coefficients, multiply out PCA basis -> normalised (log)-spectrum, shift and re-scale (log)-spectrum -> output (log)-spectrum return np . dot ( layers [ - 1 ] * self . pca_std_ + self . pca_mean_ , self . pca_transform_matrix_ ) * self . features_std_ + self . features_mean_ def predictions_np ( self , parameters_dict , ): r \"\"\" Predictions given input parameters collected in a dict. Fully implemented in Numpy. Calls ``forward_pass_np`` after ordering the input parameter dict Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: output predictions \"\"\" parameters_arr = self . dict_to_ordered_arr_np ( parameters_dict ) return self . forward_pass_np ( parameters_arr ) # 10.**predictions def ten_to_predictions_np ( self , parameters_dict , ): r \"\"\" 10^predictions given input parameters collected in a dict. Fully implemented in Numpy. It raises 10 to the output from ``forward_pass_np`` Parameters: parameters_dict (dict [numpy.ndarray]): dictionary of (arrays of) parameters Returns: numpy.ndarray: 10^output predictions \"\"\" return 10. ** self . predictions_np ( parameters_dict ) ### Infrastructure for network training ### @tf . function def compute_loss ( self , training_parameters , training_pca , ): r \"\"\" Mean squared difference Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: Tensor: mean squared difference \"\"\" return tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) @tf . function def compute_loss_and_gradients ( self , training_parameters , training_pca , ): r \"\"\" Compute mean squared difference and gradients Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference gradients (Tensor): gradients \"\"\" # compute loss on the tape with tf . GradientTape () as tape : # loss loss = tf . sqrt ( tf . reduce_mean ( tf . math . squared_difference ( self . forward_pass_tf ( training_parameters ), training_pca ))) # compute gradients gradients = tape . gradient ( loss , self . trainable_variables ) return loss , gradients def training_step ( self , training_parameters , training_pca , ): r \"\"\" Optimizes loss Parameters: training_parameters (Tensor): input parameters training_pca (Tensor): true PCA components Returns: loss (Tensor): mean squared difference \"\"\" # compute loss and gradients loss , gradients = self . compute_loss_and_gradients ( training_parameters , training_pca ) # apply gradients self . optimizer . apply_gradients ( zip ( gradients , self . trainable_variables )) return loss def training_step_with_accumulated_gradients ( self , training_parameters , training_pca , accumulation_steps = 10 , ): r \"\"\" Optimize loss breaking calculation into accumulated gradients Parameters: training_parameters (Tensor): tensor of input parameters for the network training_pca (Tensor): tensor of true PCA components accumulation_steps (int): number of accumulated gradients Returns: accumulated_loss (Tensor): mean squared difference \"\"\" # create dataset to do sub-calculations over dataset = tf . data . Dataset . from_tensor_slices (( training_parameters , training_pca )) . batch ( int ( training_pca . shape [ 0 ] / accumulation_steps )) # initialize gradients and loss (to zero) accumulated_gradients = [ tf . Variable ( tf . zeros_like ( variable ), trainable = False ) for variable in self . trainable_variables ] accumulated_loss = tf . Variable ( 0. , trainable = False ) # loop over sub-batches for training_parameters_ , training_pca_ in dataset : # calculate loss and gradients loss , gradients = self . compute_loss_and_gradients_pca ( training_parameters_ , training_pca_ ,) # update the accumulated gradients and loss for i in range ( len ( accumulated_gradients )): accumulated_gradients [ i ] . assign_add ( gradients [ i ] * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) accumulated_loss . assign_add ( loss * training_pca_ . shape [ 0 ] / training_pca . shape [ 0 ]) # apply accumulated gradients self . optimizer . apply_gradients ( zip ( accumulated_gradients , self . trainable_variables )) return accumulated_loss # ========================================== # main TRAINING function # ========================================== def train ( self , filename_saved_model , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ): r \"\"\" Train the model Parameters: filename_saved_model (str): filename tag where model will be saved validation_split (float): percentage of training data used for validation learning_rates (list [float]): learning rates for each step of learning schedule batch_sizes (list [int]): batch sizes for each step of learning schedule gradient_accumulation_steps (list [int]): batches for gradient accumulations for each step of learning schedule patience_values (list [int]): early stopping patience for each step of learning schedule max_epochs (list [int]): maximum number of epochs for each step of learning schedule \"\"\" # check correct number of steps assert len ( learning_rates ) == len ( batch_sizes ) \\ == len ( gradient_accumulation_steps ) == len ( patience_values ) == len ( max_epochs ), \\ 'Number of learning rates, batch sizes, gradient accumulation steps, patience values and max epochs are not matching!' # training start info, if verbose if self . verbose : multiline_str = \"Starting cosmopower_PCAplusNN training, \\n \" \\ f \"using { int ( 100 * validation_split ) } per cent of training samples for validation. \\n \" \\ f \"Performing { len ( learning_rates ) } learning steps, with \\n \" \\ f \" { list ( learning_rates ) } learning rates \\n \" \\ f \" { list ( batch_sizes ) } batch sizes \\n \" \\ f \" { list ( gradient_accumulation_steps ) } gradient accumulation steps \\n \" \\ f \" { list ( patience_values ) } patience values \\n \" \\ f \" { list ( max_epochs ) } max epochs \\n \" print ( multiline_str ) # casting training_parameters = tf . convert_to_tensor ( self . cp_pca . training_parameters , dtype = dtype ) training_pca = tf . convert_to_tensor ( self . cp_pca . training_pca , dtype = dtype ) # training/validation split n_validation = int ( training_parameters . shape [ 0 ] * validation_split ) n_training = training_parameters . shape [ 0 ] - n_validation # train using cooling/heating schedule for lr/batch-size for i in range ( len ( learning_rates )): print ( 'learning rate = ' + str ( learning_rates [ i ]) + ', batch size = ' + str ( batch_sizes [ i ])) # set learning rate self . optimizer . lr = learning_rates [ i ] # split into validation and training sub-sets training_selection = tf . random . shuffle ([ True ] * n_training + [ False ] * n_validation ) # create iterable dataset (given batch size) training_data = tf . data . Dataset . from_tensor_slices (( training_parameters [ training_selection ], training_pca [ training_selection ])) . shuffle ( n_training ) . batch ( batch_sizes [ i ]) # set up training loss training_loss = [ np . infty ] validation_loss = [ np . infty ] best_loss = np . infty early_stopping_counter = 0 # loop over epochs with trange ( max_epochs [ i ]) as t : for epoch in t : # loop over batches for theta , pca in training_data : # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes) if gradient_accumulation_steps [ i ] == 1 : loss = self . training_step ( theta , pca ) else : loss = self . training_step_with_accumulated_gradients ( theta , pca , accumulation_steps = gradient_accumulation_steps [ i ]) # compute validation loss at the end of the epoch validation_loss . append ( self . compute_loss ( training_parameters [ ~ training_selection ], training_pca [ ~ training_selection ]) . numpy ()) # update the progressbar t . set_postfix ( loss = validation_loss [ - 1 ]) # early stopping condition if validation_loss [ - 1 ] < best_loss : best_loss = validation_loss [ - 1 ] early_stopping_counter = 0 else : early_stopping_counter += 1 if early_stopping_counter >= patience_values [ i ]: self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Validation loss = ' + str ( best_loss )) print ( 'Model saved.' ) break self . update_emulator_parameters () self . save ( filename_saved_model ) print ( 'Reached max number of epochs. Validation loss = ' + str ( best_loss )) print ( 'Model saved.' )","title":"cosmopower_PCAplusNN"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/","text":"INTRODUCTION \u00b6 This notebook can also be run on Colab. In this notebook we will explore the main features of the emulators for cosmological power spectra implemented in the CosmoPower class cosmopower_NN . We will consider two examples: the first one is an emulator for CMB TT power spectra, the second one is an emulator of the linear matter power spectrum. Note 1 : there is, in fact, no fundamental difference in the way cosmopower_NN works for CMB and \\(P(k)\\) power spectra. Here we are explicitly showing both cases for completeness, but the internal workings of the emulator are the same and the way one obtains predictions from the emulator also does not change; Note 2 : similarly, there is no fundamental difference between different types of CMB power spectra (TT,TE,EE,PP) or linear/non linear \\(P(k)\\) . In this notebook we will consider examples involving CMB TT power spectra and linear matter power spectrum, but the same syntax can be used for all types of spectra emulated with cosmopower_NN . PRELIMINARY OPERATIONS \u00b6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os from IPython.display import display , clear_output cosmopower_NN - CMB example \u00b6 A cosmopower_NN model is a direct mapping between cosmological parameters and (log)-power spectra. Let\u2019s consider a pre-trained model emulating Cosmic Microwave Background temperature (TT) power spectra. In the CosmoPower repository we can find an example of such a pre-trained model in the trained_models folder ( cmb_TT_NN.pkl ). Loading a saved model \u00b6 Loading a saved cosmopower_NN model is as simple as instantiating a cosmopower_NN instance with: the attribute restore set to True and the attribute restore_filename pointing to the .pkl file where the model is saved ( without the .pkl suffix). import os ipynb_path = os . path . dirname ( os . path . realpath ( \"__file__\" )) import cosmopower as cp # load pre-trained NN model: maps cosmological parameters to CMB TT log-C_ell cp_nn = cp . cosmopower_NN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN' )) Exploring the model features \u00b6 Now that the model is loaded, we can explore some its features. First, let\u2019s see what are the parameters that the emulator was trained on: print ( 'emulator parameters: ' , cp_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s']) Next, let\u2019s see how the emulator samples the \\(\\ell\\) multipoles for this CMB case: print ( 'sampled multipoles: ' , cp_nn . modes ) print ( 'number of multipoles: ' , cp_nn . n_modes ) sampled multipoles: [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 The model is a direct neural network mapping 6 parameters into 2507 multipoles. Let\u2019s find out its internal architecture: print ( 'hidden layers: ' , cp_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512, 512]) The model has 4 hidden layers, each with 512 nodes. Obtaining predictions \u00b6 Now let\u2019s explore how cosmopower_NN produces power spectra predictions for a given set of input parameters. All CosmoPower models take in input a Python dict of parameters, for example: # # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'tau_reio' : [ 0.055 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], } The reason for using input dictionaries is that the user does not need to be concerned with ordering the input parameters. The loaded model is an emulator of log -power spectra. This was done to reduce the dynamic range of the training set, which helps considerably the emulator during training. Therefore, a forward pass of the input parameters on the trained network (with the method predictions_np ) would result in a prediction for the log-power spectrum: # predictions (= forward pass thorugh the network) log_spectrum_cosmopower_NN = cp_nn . predictions_np ( params ) Sometimes it is convenient to output log-spectra, e.g. for interpolation purposes. However, if the user is only interested in spectra, the conversion from log-spectra to spectra can also be done internally by cosmopower_NN , with the method ten_to_predictions_np : # predictions (= forward pass thorugh the network) -> 10^predictions spectrum_cosmopower_NN = cp_nn . ten_to_predictions_np ( params ) COMPARING WITH CLASS \u00b6 Let\u2019s compare the prediction for the TT power spectrum from CosmoPower with that from the Boltzmann code Class . We first need to clone the CLASS repository , compile and install the code: ! pip install cython ! git clone https : // github . com / lesgourg / class_public % cd class_public ! make clean ! make % cd python ! python setup . py build ! python setup . py install The call to Class to obtain the same power spectrum prediction obtained from CosmoPower is as follows: from classy import Class cosmo = Class () # Define cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl lCl' , 'l_max_scalars' : 2508 , 'non linear' : 'hmcode' , 'nonlinear_min_k_max' : 20 , 'lensing' : 'yes' , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'tau_reio' : 0.055 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } cosmo . set ( params ) cosmo . compute () cls = cosmo . lensed_cl ( lmax = 2508 ) spectrum_class = cls [ 'tt' ][ 2 :] cosmo . struct_cleanup () cosmo . empty () Now let\u2019s plot a comparison between the CosmoPower and Class predictions: ell_modes = cp_nn . modes fig = plt . figure ( figsize = ( 20 , 10 )) true = spectrum_class * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) pred = spectrum_cosmopower_NN [ 0 ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) plt . semilogx ( ell_modes , true , 'red' , label = 'CLASS' , linewidth = 5 ) plt . semilogx ( ell_modes , pred , 'blue' , label = 'COSMOPOWER NN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 ) BATCH PREDICTIONS \u00b6 So far we considered predictions for a single set of input parameters. However, to fully harvest the computational power of neural networks, it is often convenient to consider batch predictions for multiple sets of input parameters. These can be easily obtained from cosmopower_NN by feeding the emulator with a dict of np.arrays of parameters. For example, let\u2019s create 10 random sets of cosmological parameters: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) tau_reio = np . random . uniform ( low = 0.01 , high = 0.1 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) Now let\u2019s collect these 10 sets of cosmological parameters into a dict of np.arrays : # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'tau_reio' : tau_reio , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , } We can now simultaneously obtain predictions for all of the 10 parameter sets. Note that the syntax is unchanged w.r.t. the single-set case! # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_NN = cp_nn . ten_to_predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_NN [ i ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $ \\\\ tau$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'tau_reio' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ]) plt . semilogx ( ell_modes , pred , label = label ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 ) cosmopower_NN - \\(P(k)\\) example \u00b6 As explained above, there is no fundamental difference between the way cosmopower_NN works for CMB and matter power spectra. For this reason here we will simply run the same code cells above, this time for a linear \\(P(k)\\) emulator. Note 1 : the sampled modes here are \\(k\\) -modes. Note 2 : the units are Mpc \\(^{-1}\\) for the \\(k\\) -modes and Mpc \\(^3\\) for \\(P(k)\\) . Note 3 : the redshift \\(z\\) is treated as an additional parameter fed to cosmopower_NN . # load pre-trained NN model: maps cosmological parameters to linear log-P(k) cp_nn = cp . cosmopower_NN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/PK/PKLIN_NN' )) print ( 'emulator parameters: ' , cp_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'n_s', 'ln10^{10}A_s', 'z']) print ( 'sampled k-modes: ' , cp_nn . modes ) print ( 'number of k-modes: ' , cp_nn . n_modes ) print ( 'hidden layers: ' , cp_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512]) # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], 'z' : [ 0.5 ], } # predictions (= forward pass thorugh the network) log_spectrum_cosmopower_NN = cp_nn . predictions_np ( params ) # predictions (= forward pass thorugh the network) -> 10^predictions spectrum_cosmopower_NN = cp_nn . ten_to_predictions_np ( params )[ 0 ] COMPARING with CLASS \u00b6 k_modes = cp_nn . modes cosmo = Class () # Define your cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl mPk' , 'z_max_pk' : 5 , 'P_k_max_1/Mpc' : 10. , 'nonlinear_min_k_max' : 100. , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } # Set the parameters to the cosmological code cosmo . set ( params ) cosmo . compute () z = 0.5 spectrum_class = np . array ([ cosmo . pk ( ki , z ) for ki in k_modes ]) pred = spectrum_cosmopower_NN true = spectrum_class fig = plt . figure ( figsize = ( 20 , 10 )) plt . loglog ( k_modes , true , 'red' , linewidth = 5 , label = 'CLASS' ) plt . loglog ( k_modes , pred , 'blue' , label = 'COSMOPOWER NN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$k$ [Mpc$^{-1}]$' , fontsize = 20 ) plt . ylabel ( '$P_{\\mathrm {LIN} }(k) [\\mathrm {Mpc} ^3]$' , fontsize = 20 ) plt . legend ( fontsize = 20 ) Batch predictions work the same way as in the CMB case: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) z = np . random . uniform ( low = 0.0 , high = 5.0 , size = ( 10 ,)) # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , 'z' : z , } # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_NN = cp_nn . ten_to_predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_NN [ i ] label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} , $z$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ], batch_params [ 'z' ][ i ]) plt . loglog ( k_modes , pred , label = label ) plt . xlabel ( '$k [\\mathrm{{Mpc}}^{{-1}}]$' , fontsize = '30' ) plt . ylabel ( '$P(k)_{{\\mathrm{{LIN}}}} [\\mathrm{{Mpc}}^{{3}}]$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"NN emulation"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#introduction","text":"This notebook can also be run on Colab. In this notebook we will explore the main features of the emulators for cosmological power spectra implemented in the CosmoPower class cosmopower_NN . We will consider two examples: the first one is an emulator for CMB TT power spectra, the second one is an emulator of the linear matter power spectrum. Note 1 : there is, in fact, no fundamental difference in the way cosmopower_NN works for CMB and \\(P(k)\\) power spectra. Here we are explicitly showing both cases for completeness, but the internal workings of the emulator are the same and the way one obtains predictions from the emulator also does not change; Note 2 : similarly, there is no fundamental difference between different types of CMB power spectra (TT,TE,EE,PP) or linear/non linear \\(P(k)\\) . In this notebook we will consider examples involving CMB TT power spectra and linear matter power spectrum, but the same syntax can be used for all types of spectra emulated with cosmopower_NN .","title":"INTRODUCTION"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#preliminary-operations","text":"import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os from IPython.display import display , clear_output","title":"PRELIMINARY OPERATIONS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#cosmopower_nn-cmb-example","text":"A cosmopower_NN model is a direct mapping between cosmological parameters and (log)-power spectra. Let\u2019s consider a pre-trained model emulating Cosmic Microwave Background temperature (TT) power spectra. In the CosmoPower repository we can find an example of such a pre-trained model in the trained_models folder ( cmb_TT_NN.pkl ).","title":"cosmopower_NN - CMB example"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#loading-a-saved-model","text":"Loading a saved cosmopower_NN model is as simple as instantiating a cosmopower_NN instance with: the attribute restore set to True and the attribute restore_filename pointing to the .pkl file where the model is saved ( without the .pkl suffix). import os ipynb_path = os . path . dirname ( os . path . realpath ( \"__file__\" )) import cosmopower as cp # load pre-trained NN model: maps cosmological parameters to CMB TT log-C_ell cp_nn = cp . cosmopower_NN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN' ))","title":"Loading a saved model"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#exploring-the-model-features","text":"Now that the model is loaded, we can explore some its features. First, let\u2019s see what are the parameters that the emulator was trained on: print ( 'emulator parameters: ' , cp_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s']) Next, let\u2019s see how the emulator samples the \\(\\ell\\) multipoles for this CMB case: print ( 'sampled multipoles: ' , cp_nn . modes ) print ( 'number of multipoles: ' , cp_nn . n_modes ) sampled multipoles: [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 The model is a direct neural network mapping 6 parameters into 2507 multipoles. Let\u2019s find out its internal architecture: print ( 'hidden layers: ' , cp_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512, 512]) The model has 4 hidden layers, each with 512 nodes.","title":"Exploring the model features"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#obtaining-predictions","text":"Now let\u2019s explore how cosmopower_NN produces power spectra predictions for a given set of input parameters. All CosmoPower models take in input a Python dict of parameters, for example: # # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'tau_reio' : [ 0.055 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], } The reason for using input dictionaries is that the user does not need to be concerned with ordering the input parameters. The loaded model is an emulator of log -power spectra. This was done to reduce the dynamic range of the training set, which helps considerably the emulator during training. Therefore, a forward pass of the input parameters on the trained network (with the method predictions_np ) would result in a prediction for the log-power spectrum: # predictions (= forward pass thorugh the network) log_spectrum_cosmopower_NN = cp_nn . predictions_np ( params ) Sometimes it is convenient to output log-spectra, e.g. for interpolation purposes. However, if the user is only interested in spectra, the conversion from log-spectra to spectra can also be done internally by cosmopower_NN , with the method ten_to_predictions_np : # predictions (= forward pass thorugh the network) -> 10^predictions spectrum_cosmopower_NN = cp_nn . ten_to_predictions_np ( params )","title":"Obtaining predictions"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#comparing-with-class","text":"Let\u2019s compare the prediction for the TT power spectrum from CosmoPower with that from the Boltzmann code Class . We first need to clone the CLASS repository , compile and install the code: ! pip install cython ! git clone https : // github . com / lesgourg / class_public % cd class_public ! make clean ! make % cd python ! python setup . py build ! python setup . py install The call to Class to obtain the same power spectrum prediction obtained from CosmoPower is as follows: from classy import Class cosmo = Class () # Define cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl lCl' , 'l_max_scalars' : 2508 , 'non linear' : 'hmcode' , 'nonlinear_min_k_max' : 20 , 'lensing' : 'yes' , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'tau_reio' : 0.055 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } cosmo . set ( params ) cosmo . compute () cls = cosmo . lensed_cl ( lmax = 2508 ) spectrum_class = cls [ 'tt' ][ 2 :] cosmo . struct_cleanup () cosmo . empty () Now let\u2019s plot a comparison between the CosmoPower and Class predictions: ell_modes = cp_nn . modes fig = plt . figure ( figsize = ( 20 , 10 )) true = spectrum_class * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) pred = spectrum_cosmopower_NN [ 0 ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) plt . semilogx ( ell_modes , true , 'red' , label = 'CLASS' , linewidth = 5 ) plt . semilogx ( ell_modes , pred , 'blue' , label = 'COSMOPOWER NN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"COMPARING WITH CLASS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#batch-predictions","text":"So far we considered predictions for a single set of input parameters. However, to fully harvest the computational power of neural networks, it is often convenient to consider batch predictions for multiple sets of input parameters. These can be easily obtained from cosmopower_NN by feeding the emulator with a dict of np.arrays of parameters. For example, let\u2019s create 10 random sets of cosmological parameters: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) tau_reio = np . random . uniform ( low = 0.01 , high = 0.1 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) Now let\u2019s collect these 10 sets of cosmological parameters into a dict of np.arrays : # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'tau_reio' : tau_reio , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , } We can now simultaneously obtain predictions for all of the 10 parameter sets. Note that the syntax is unchanged w.r.t. the single-set case! # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_NN = cp_nn . ten_to_predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_NN [ i ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $ \\\\ tau$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'tau_reio' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ]) plt . semilogx ( ell_modes , pred , label = label ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"BATCH PREDICTIONS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#cosmopower_nn-pk-example","text":"As explained above, there is no fundamental difference between the way cosmopower_NN works for CMB and matter power spectra. For this reason here we will simply run the same code cells above, this time for a linear \\(P(k)\\) emulator. Note 1 : the sampled modes here are \\(k\\) -modes. Note 2 : the units are Mpc \\(^{-1}\\) for the \\(k\\) -modes and Mpc \\(^3\\) for \\(P(k)\\) . Note 3 : the redshift \\(z\\) is treated as an additional parameter fed to cosmopower_NN . # load pre-trained NN model: maps cosmological parameters to linear log-P(k) cp_nn = cp . cosmopower_NN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/PK/PKLIN_NN' )) print ( 'emulator parameters: ' , cp_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'n_s', 'ln10^{10}A_s', 'z']) print ( 'sampled k-modes: ' , cp_nn . modes ) print ( 'number of k-modes: ' , cp_nn . n_modes ) print ( 'hidden layers: ' , cp_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512]) # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], 'z' : [ 0.5 ], } # predictions (= forward pass thorugh the network) log_spectrum_cosmopower_NN = cp_nn . predictions_np ( params ) # predictions (= forward pass thorugh the network) -> 10^predictions spectrum_cosmopower_NN = cp_nn . ten_to_predictions_np ( params )[ 0 ]","title":"cosmopower_NN - \\(P(k)\\) example"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_NN/getting_started_with_cosmopower_NN/#comparing-with-class_1","text":"k_modes = cp_nn . modes cosmo = Class () # Define your cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl mPk' , 'z_max_pk' : 5 , 'P_k_max_1/Mpc' : 10. , 'nonlinear_min_k_max' : 100. , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } # Set the parameters to the cosmological code cosmo . set ( params ) cosmo . compute () z = 0.5 spectrum_class = np . array ([ cosmo . pk ( ki , z ) for ki in k_modes ]) pred = spectrum_cosmopower_NN true = spectrum_class fig = plt . figure ( figsize = ( 20 , 10 )) plt . loglog ( k_modes , true , 'red' , linewidth = 5 , label = 'CLASS' ) plt . loglog ( k_modes , pred , 'blue' , label = 'COSMOPOWER NN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$k$ [Mpc$^{-1}]$' , fontsize = 20 ) plt . ylabel ( '$P_{\\mathrm {LIN} }(k) [\\mathrm {Mpc} ^3]$' , fontsize = 20 ) plt . legend ( fontsize = 20 ) Batch predictions work the same way as in the CMB case: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) z = np . random . uniform ( low = 0.0 , high = 5.0 , size = ( 10 ,)) # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , 'z' : z , } # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_NN = cp_nn . ten_to_predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_NN [ i ] label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} , $z$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ], batch_params [ 'z' ][ i ]) plt . loglog ( k_modes , pred , label = label ) plt . xlabel ( '$k [\\mathrm{{Mpc}}^{{-1}}]$' , fontsize = '30' ) plt . ylabel ( '$P(k)_{{\\mathrm{{LIN}}}} [\\mathrm{{Mpc}}^{{3}}]$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"COMPARING with CLASS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/","text":"This notebook can also be run on Colab INTRODUCTION \u00b6 In this notebook we will explore the main features of the emulators for cosmological power spectra implemented in the CosmoPower class cosmopower_PCAplusNN . We will consider the example of emulating CMB TE power spectra. Note 1 : there is no fundamental difference in the way cosmopower_PCAplusNN works for CMB and \\(P(k)\\) power spectra. The internal workings of the emulator are the same and the way one obtains predictions from the emulator also does not change. Hence, if you have a cosmopower_PCAplusNN emulator for the matter power spectrum, you can just run this notebook to explore its features. Note 2 : similarly, there is no fundamental difference between different types of CMB power spectra (TT,TE,EE,PP) or linear/non linear \\(P(k)\\) . In this notebook we will consider examples involving CMB TE power spectra, but the same syntax can be used for all types of spectra emulated with cosmopower_PCAplusNN . PRELIMINARY OPERATIONS \u00b6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os from IPython.display import display , clear_output We will set the random seed in this notebook, for reproducibility of results. # setting the seed for reproducibility np . random . seed ( 1 ) tf . random . set_seed ( 2 ) cosmopower_PCAplusNN \u00b6 A cosmopower_PCAplusNN model is a mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Let\u2019s consider a pre-trained model emulating Cosmic Microwave Background temperature-polarization (TE) power spectra. In the CosmoPower repository we can find an example of such a pre-trained model in the trained_models folder ( cmb_TE_PCAplusNN.pkl ). Loading a saved model \u00b6 Loading a saved cosmopower_PCAplusNN model is as simple as instantiating a cosmopower_PCAplusNN instance with: the attribute restore set to True and the attribute restore_filename pointing to the .pkl file where the model is saved ( without the .pkl suffix). import os ipynb_path = os . path . dirname ( os . path . realpath ( \"__file__\" )) import cosmopower as cp # load pre-trained NN model: maps cosmological parameters to CMB TE C_ell cp_pca_nn = cp . cosmopower_PCAplusNN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/CMB/cmb_TE_PCAplusNN' )) Exploring the model features \u00b6 Now that the model is loaded, we can explore some its features. First, let\u2019s see what are the parameters that the emulator was trained on: print ( 'emulator parameters: ' , cp_pca_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s']) Next, let\u2019s see how the emulator samples the mutipoles \\(\\ell\\) for this CMB case: print ( 'sampled multipoles: ' , cp_pca_nn . modes ) print ( 'number of multipoles: ' , cp_pca_nn . n_modes ) sampled multipoles: [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 We can also have a look at how many PCA coefficients the emulator uses to compress the spectra: print ( 'number of PCA coefficients: ' , cp_pca_nn . n_pcas ) number of PCA coefficients: 512 The model is a neural network mapping 6 parameters into 512 PCA coefficients. The PCA compression can be inverted to return a prediction for a spectrum sampled at 2507 multipoles. Now Let\u2019s find out the internal architecture of the model: print ( 'hidden layers: ' , cp_pca_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512, 512]) The model has 4 hidden layers, each with 512 nodes. Obtaining predictions \u00b6 Now let\u2019s explore how cosmopower_PCAplusNN produces power spectra predictions for a given set of input parameters. All CosmoPower models take in input a Python dict of parameters, for example: # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'tau_reio' : [ 0.055 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], } The reason for using input dictionaries is that the user does not need to be concerned with ordering the input parameters. The loaded model is an emulator of power spectra, not log-power spectra. Training the model on the logarithm of thte power spectra is always recommended, as it reduces the dynamic range to be learnt. However, in the case of TE spectra it is not possible to take the logarithm of the spectra. Therefore, a forward pass of the input parameters on the trained network (with the method predictions_np ) would result in a prediction for the power spectrum directly (not for the log-power spectrum): # predictions (= forward pass through the network) spectrum_cosmopower_PCAplusNN = cp_pca_nn . predictions_np ( params ) COMPARING WITH CLASS \u00b6 Let\u2019s compare the prediction for the TE power spectrum from CosmoPower with that from the Boltzmann code Class . We first need to clone the CLASS repository, compile and install the code: ! pip install cython ! git clone https : // github . com / lesgourg / class_public % cd class_public ! make clean ! make % cd python ! python setup . py build ! python setup . py install The call to Class to obtain the same power spectrum prediction obtained from CosmoPower is as follows: from classy import Class cosmo = Class () # Define cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl pCl lCl' , 'l_max_scalars' : 2508 , 'non linear' : 'hmcode' , 'nonlinear_min_k_max' : 20 , 'lensing' : 'yes' , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'tau_reio' : 0.055 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } cosmo . set ( params ) cosmo . compute () cls = cosmo . lensed_cl ( lmax = 2508 ) spectrum_class = cls [ 'te' ][ 2 :] cosmo . struct_cleanup () cosmo . empty () Now let\u2019s plot a comparison between the CosmoPower and Class predictions: ell_modes = cp_pca_nn . modes fig = plt . figure ( figsize = ( 20 , 10 )) true = spectrum_class * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) pred = spectrum_cosmopower_PCAplusNN [ 0 ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) plt . semilogx ( ell_modes , true , 'red' , label = 'CLASS' , linewidth = 5 ) plt . semilogx ( ell_modes , pred , 'blue' , label = 'COSMOPOWER PCAplusNN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 ) BATCH PREDICTIONS \u00b6 So far we considered predictions for a single set of input parameters. However, to fully harvest the computational power of neural networks, it is often convenient to consider batch predictions for multiple sets of input parameters. These can be easily obtained from cosmopower_PCAplusNN by feeding the emulator with a dict of np.arrays of parameters. For example, let\u2019s create 10 random sets of cosmological parameters: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) tau_reio = np . random . uniform ( low = 0.01 , high = 0.1 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) Now let\u2019s collect these 10 sets of cosmological parameters into a dict of np.arrays : # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'tau_reio' : tau_reio , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , } We can now simultaneously obtain predictions for all of the 10 parameter sets. Note that the syntax is unchanged w.r.t. the single-set case! # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_PCAplusNN = cp_pca_nn . predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_PCAplusNN [ i ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $ \\\\ tau$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'tau_reio' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ]) plt . semilogx ( ell_modes , pred , label = label ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"PCA+NN emulation"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#introduction","text":"In this notebook we will explore the main features of the emulators for cosmological power spectra implemented in the CosmoPower class cosmopower_PCAplusNN . We will consider the example of emulating CMB TE power spectra. Note 1 : there is no fundamental difference in the way cosmopower_PCAplusNN works for CMB and \\(P(k)\\) power spectra. The internal workings of the emulator are the same and the way one obtains predictions from the emulator also does not change. Hence, if you have a cosmopower_PCAplusNN emulator for the matter power spectrum, you can just run this notebook to explore its features. Note 2 : similarly, there is no fundamental difference between different types of CMB power spectra (TT,TE,EE,PP) or linear/non linear \\(P(k)\\) . In this notebook we will consider examples involving CMB TE power spectra, but the same syntax can be used for all types of spectra emulated with cosmopower_PCAplusNN .","title":"INTRODUCTION"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#preliminary-operations","text":"import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os from IPython.display import display , clear_output We will set the random seed in this notebook, for reproducibility of results. # setting the seed for reproducibility np . random . seed ( 1 ) tf . random . set_seed ( 2 )","title":"PRELIMINARY OPERATIONS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#cosmopower_pcaplusnn","text":"A cosmopower_PCAplusNN model is a mapping between cosmological parameters and PCA coefficients of (log)-power spectra. Let\u2019s consider a pre-trained model emulating Cosmic Microwave Background temperature-polarization (TE) power spectra. In the CosmoPower repository we can find an example of such a pre-trained model in the trained_models folder ( cmb_TE_PCAplusNN.pkl ).","title":"cosmopower_PCAplusNN"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#loading-a-saved-model","text":"Loading a saved cosmopower_PCAplusNN model is as simple as instantiating a cosmopower_PCAplusNN instance with: the attribute restore set to True and the attribute restore_filename pointing to the .pkl file where the model is saved ( without the .pkl suffix). import os ipynb_path = os . path . dirname ( os . path . realpath ( \"__file__\" )) import cosmopower as cp # load pre-trained NN model: maps cosmological parameters to CMB TE C_ell cp_pca_nn = cp . cosmopower_PCAplusNN ( restore = True , restore_filename = os . path . join ( ipynb_path , '../../cosmopower/trained_models/CP_paper/CMB/cmb_TE_PCAplusNN' ))","title":"Loading a saved model"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#exploring-the-model-features","text":"Now that the model is loaded, we can explore some its features. First, let\u2019s see what are the parameters that the emulator was trained on: print ( 'emulator parameters: ' , cp_pca_nn . parameters ) emulator parameters: ListWrapper(['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s']) Next, let\u2019s see how the emulator samples the mutipoles \\(\\ell\\) for this CMB case: print ( 'sampled multipoles: ' , cp_pca_nn . modes ) print ( 'number of multipoles: ' , cp_pca_nn . n_modes ) sampled multipoles: [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 We can also have a look at how many PCA coefficients the emulator uses to compress the spectra: print ( 'number of PCA coefficients: ' , cp_pca_nn . n_pcas ) number of PCA coefficients: 512 The model is a neural network mapping 6 parameters into 512 PCA coefficients. The PCA compression can be inverted to return a prediction for a spectrum sampled at 2507 multipoles. Now Let\u2019s find out the internal architecture of the model: print ( 'hidden layers: ' , cp_pca_nn . n_hidden ) hidden layers: ListWrapper([512, 512, 512, 512]) The model has 4 hidden layers, each with 512 nodes.","title":"Exploring the model features"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#obtaining-predictions","text":"Now let\u2019s explore how cosmopower_PCAplusNN produces power spectra predictions for a given set of input parameters. All CosmoPower models take in input a Python dict of parameters, for example: # create a dict of cosmological parameters params = { 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.113 ], 'h' : [ 0.7 ], 'tau_reio' : [ 0.055 ], 'n_s' : [ 0.96 ], 'ln10^ {10} A_s' : [ 3.07 ], } The reason for using input dictionaries is that the user does not need to be concerned with ordering the input parameters. The loaded model is an emulator of power spectra, not log-power spectra. Training the model on the logarithm of thte power spectra is always recommended, as it reduces the dynamic range to be learnt. However, in the case of TE spectra it is not possible to take the logarithm of the spectra. Therefore, a forward pass of the input parameters on the trained network (with the method predictions_np ) would result in a prediction for the power spectrum directly (not for the log-power spectrum): # predictions (= forward pass through the network) spectrum_cosmopower_PCAplusNN = cp_pca_nn . predictions_np ( params )","title":"Obtaining predictions"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#comparing-with-class","text":"Let\u2019s compare the prediction for the TE power spectrum from CosmoPower with that from the Boltzmann code Class . We first need to clone the CLASS repository, compile and install the code: ! pip install cython ! git clone https : // github . com / lesgourg / class_public % cd class_public ! make clean ! make % cd python ! python setup . py build ! python setup . py install The call to Class to obtain the same power spectrum prediction obtained from CosmoPower is as follows: from classy import Class cosmo = Class () # Define cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl pCl lCl' , 'l_max_scalars' : 2508 , 'non linear' : 'hmcode' , 'nonlinear_min_k_max' : 20 , 'lensing' : 'yes' , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : 0.0225 , 'omega_cdm' : 0.113 , 'h' : 0.7 , 'tau_reio' : 0.055 , 'n_s' : 0.96 , 'ln10^ {10} A_s' : 3.07 , } cosmo . set ( params ) cosmo . compute () cls = cosmo . lensed_cl ( lmax = 2508 ) spectrum_class = cls [ 'te' ][ 2 :] cosmo . struct_cleanup () cosmo . empty () Now let\u2019s plot a comparison between the CosmoPower and Class predictions: ell_modes = cp_pca_nn . modes fig = plt . figure ( figsize = ( 20 , 10 )) true = spectrum_class * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) pred = spectrum_cosmopower_PCAplusNN [ 0 ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) plt . semilogx ( ell_modes , true , 'red' , label = 'CLASS' , linewidth = 5 ) plt . semilogx ( ell_modes , pred , 'blue' , label = 'COSMOPOWER PCAplusNN' , linewidth = 5 , linestyle = '--' ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"COMPARING WITH CLASS"},{"location":"tutorial/getting_started/getting_started_with_cosmopower_PCAplusNN/getting_started_with_cosmopower_PCAplusNN/#batch-predictions","text":"So far we considered predictions for a single set of input parameters. However, to fully harvest the computational power of neural networks, it is often convenient to consider batch predictions for multiple sets of input parameters. These can be easily obtained from cosmopower_PCAplusNN by feeding the emulator with a dict of np.arrays of parameters. For example, let\u2019s create 10 random sets of cosmological parameters: omega_b = np . random . uniform ( low = 0.01875 , high = 0.02625 , size = ( 10 ,)) omega_cdm = np . random . uniform ( low = 0.05 , high = 0.255 , size = ( 10 ,)) h = np . random . uniform ( low = 0.64 , high = 0.82 , size = ( 10 ,)) tau_reio = np . random . uniform ( low = 0.01 , high = 0.1 , size = ( 10 ,)) n_s = np . random . uniform ( low = 0.84 , high = 1.1 , size = ( 10 ,)) lnAs = np . random . uniform ( low = 1.61 , high = 3.91 , size = ( 10 ,)) Now let\u2019s collect these 10 sets of cosmological parameters into a dict of np.arrays : # create a dict of cosmological parameters batch_params = { 'omega_b' : omega_b , 'omega_cdm' : omega_cdm , 'h' : h , 'tau_reio' : tau_reio , 'n_s' : n_s , 'ln10^ {10} A_s' : lnAs , } We can now simultaneously obtain predictions for all of the 10 parameter sets. Note that the syntax is unchanged w.r.t. the single-set case! # predictions (= forward pass thorugh the network) -> 10^predictions batch_spectra_cosmopower_PCAplusNN = cp_pca_nn . predictions_np ( batch_params ) fig = plt . figure ( figsize = ( 30 , 20 )) for i in range ( 10 ): pred = batch_spectra_cosmopower_PCAplusNN [ i ] * ell_modes * ( ell_modes + 1 ) / ( 2. * np . pi ) label = '$\\omega_{{\\mathrm{{b}}}}$: {:1.3f} , $\\omega_{{\\mathrm{{cdm}}}}$: {:1.2f} , $h$: {:1.2f} , $ \\\\ tau$: {:1.2f} , $n_s$: {:1.2f} , $ \\\\ mathrm{{ln}} 10^{{10}}A_s$: {:1.2f} ' . format ( batch_params [ 'omega_b' ][ i ], batch_params [ 'omega_cdm' ][ i ], batch_params [ 'h' ][ i ], batch_params [ 'tau_reio' ][ i ], batch_params [ 'n_s' ][ i ], batch_params [ 'ln10^ {10} A_s' ][ i ]) plt . semilogx ( ell_modes , pred , label = label ) plt . xlabel ( '$\\ell$' , fontsize = '30' ) plt . ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = '30' ) plt . legend ( fontsize = 20 )","title":"BATCH PREDICTIONS"},{"location":"tutorial/likelihoods/tf_planck2018_lite/","text":"The notebook tf_planck2018_lite.ipynb shows an example of how to run a complete inference pipeline with power spectra sourced from CosmoPower . The notebooks runs a version of the Planck 2018 lite likelihood rewritten to be fully implemented in TensorFlow : tf_planck2018_lite.py . The lite version of the Planck likelihood is pre-marginalised over a set of nuisance parameters. This TensorFlow version of the Planck lite likelihood, provided as part of CosmoPower , is an adaptation for TensorFlow of the planck-lite-py likelihood written by H. Prince and J. Dunkley. If you use tf_planck2018_lite , in addition to the CosmoPower release paper please also cite Prince & Dunkley (2019) and Planck (2018) . The notebook tf_planck2018_lite.ipynb can also be run on Colab tf_planck2018_lite instantiation \u00b6 Her we will simply show how to instantiate the tf_planck2018_lite likelihood, referring to the tf_planck2018_lite.ipynb notebook for a more detailed example of how to run it for inference. The tf_planck2018_lite likelihood requires emulators for the TT, TE, EE power spectra. In the tf_planck2018_lite.ipynb notebook we use the pre-trained models from the CosmoPower release paper , available in the CosmoPower repository . To create an instance of tf_planck2018_lite , we import CosmoPower and remember to input: a path to the tf_planck2018_lite likelihood. It will be used to access the Planck data; parameters of the analysis, as well as their priors; the CosmoPower emulators. import cosmopower as cp # CosmoPower emulators tt_emu_model = cp . cosmopower_NN ( restore = True , restore_filename = 'cmb_TT_NN' ) te_emu_model = cp . cosmopower_PCAplusNN ( restore = True , restore_filename = 'cmb_TE_PCAplusNN' ) ee_emu_model = cp . cosmopower_NN ( restore = True , restore_filename = 'cmb_EE_NN' ) # path to the tf_planck2018_lite likelihood tf_planck2018_lite_path = '/path/to/cosmopower/likelihoods/tf_planck2018_lite/' # parameters of the analysis, and their priors parameters_and_priors = { 'omega_b' : [ 0.001 , 0.04 , 'uniform' ], 'omega_cdm' : [ 0.005 , 0.99 , 'uniform' ], 'h' : [ 0.2 , 1.0 , 'uniform' ], 'tau_reio' : [ 0.01 , 0.8 , 'uniform' ], 'n_s' : [ 0.9 , 1.1 , 'uniform' ], 'ln10^ {10} A_s' : [ 1.61 , 3.91 , 'uniform' ], 'A_planck' : [ 1.0 , 0.01 , 'gaussian' ], } # instantiation tf_planck = cp . tf_planck2018_lite ( parameters = parameters_and_priors , tf_planck2018_lite_path = tf_planck2018_lite_path , tt_emu_model = tt_emu_model , te_emu_model = te_emu_model , ee_emu_model = ee_emu_model )","title":"TensorFlow Planck-lite 2018 likelihood"},{"location":"tutorial/likelihoods/tf_planck2018_lite/#tf_planck2018_lite-instantiation","text":"Her we will simply show how to instantiate the tf_planck2018_lite likelihood, referring to the tf_planck2018_lite.ipynb notebook for a more detailed example of how to run it for inference. The tf_planck2018_lite likelihood requires emulators for the TT, TE, EE power spectra. In the tf_planck2018_lite.ipynb notebook we use the pre-trained models from the CosmoPower release paper , available in the CosmoPower repository . To create an instance of tf_planck2018_lite , we import CosmoPower and remember to input: a path to the tf_planck2018_lite likelihood. It will be used to access the Planck data; parameters of the analysis, as well as their priors; the CosmoPower emulators. import cosmopower as cp # CosmoPower emulators tt_emu_model = cp . cosmopower_NN ( restore = True , restore_filename = 'cmb_TT_NN' ) te_emu_model = cp . cosmopower_PCAplusNN ( restore = True , restore_filename = 'cmb_TE_PCAplusNN' ) ee_emu_model = cp . cosmopower_NN ( restore = True , restore_filename = 'cmb_EE_NN' ) # path to the tf_planck2018_lite likelihood tf_planck2018_lite_path = '/path/to/cosmopower/likelihoods/tf_planck2018_lite/' # parameters of the analysis, and their priors parameters_and_priors = { 'omega_b' : [ 0.001 , 0.04 , 'uniform' ], 'omega_cdm' : [ 0.005 , 0.99 , 'uniform' ], 'h' : [ 0.2 , 1.0 , 'uniform' ], 'tau_reio' : [ 0.01 , 0.8 , 'uniform' ], 'n_s' : [ 0.9 , 1.1 , 'uniform' ], 'ln10^ {10} A_s' : [ 1.61 , 3.91 , 'uniform' ], 'A_planck' : [ 1.0 , 0.01 , 'gaussian' ], } # instantiation tf_planck = cp . tf_planck2018_lite ( parameters = parameters_and_priors , tf_planck2018_lite_path = tf_planck2018_lite_path , tt_emu_model = tt_emu_model , te_emu_model = te_emu_model , ee_emu_model = ee_emu_model )","title":"tf_planck2018_lite instantiation"},{"location":"tutorial/training/NN/training_NN/","text":"This section is an excerpt from the notebook explaining how to train a cosmopower_NN model . The user is strongly encouraged to run that notebook in its entirety, . Here we will consider only the most important parts to understand how the training works. PARAMETER FILES \u00b6 Let\u2019s take a look at the content of the parameter files that are read by CosmoPower during training. training_parameters = np . load ( './camb_tt_training_params.npz' ) training_parameters is a dict of np.arrays . There is a dict key for each of the parameters the emulator will be trained on: print ( training_parameters . files ) ['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s'] Each of these keys has a np.array of values: the number of elements in the array is the number of training samples in our set. For example: print ( training_parameters [ 'omega_b' ]) print ( 'number of training samples: ' , len ( training_parameters [ 'omega_b' ])) # same for all of the other parameters [0.03959524 0.03194429 0.03467569 ... 0.03958164 0.02816863 0.01942146] number of training samples: 42614 FEATURE FILES \u00b6 Now let\u2019s take a look at the \u201cfeatures\u201d. With features here we refer to the predictions of the neural network: these may be spectra or log-spectra values. For example, in this notebook we are emulating values of the TT log-power spectra. These are sampled at each multipole \\(\\ell\\) in the range \\([2, \\dots 2508]\\) , hence our training sample for each log-spectrum will be a 2507-dimensional array. The corresponding .npz files contain a dict with two keys: training_features = np . load ( './camb_tt_training_log_spectra.npz' ) print ( training_features . files ) ['modes', 'features'] The first key, modes , contains a np.array of the sampled Fourier modes (multipoles, in this CMB case): print ( training_features [ 'modes' ]) print ( 'number of multipoles: ' , len ( training_features [ 'modes' ])) [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 The second key, features , has values equal to the actual values of the log-spectra. These are collected in a np.array of shape (number of training samples, number of Fourier modes): training_log_spectra = training_features [ 'features' ] print ( '(number of training samples, number of ell modes): ' , training_log_spectra . shape ) (number of training samples, number of ell modes): (42614, 2507) The files for the testing samples have the same type of content. testing_params = np . load ( './camb_tt_testing_params.npz' ) testing_spectra = 10. ** ( np . load ( './camb_tt_testing_log_spectra.npz' )[ 'features' ]) cosmopower_NN INSTANTIATION \u00b6 We will now create an instance of the cosmopower_NN class . In order to instantiate the class, we need to first define some of the key aspects of our model. PARAMETERS \u00b6 Let\u2019s start by defining the parameters of our model. cosmopower_NN will take in input a set of these parameters for each prediction. If, for example, we want to emulate over a set of 6 standard \\(\\Lambda\\) CDM parameters, \\(\\omega_{\\mathrm{b}}, \\omega_{\\mathrm{cdm}}, h, \\tau, n_s, \\ln10^{10}A_s\\) we need to create a list with the names of all of these parameters, in arbitrary order: model_parameters = [ 'h' , 'tau_reio' , 'omega_b' , 'n_s' , 'ln10^ {10} A_s' , 'omega_cdm' , ] This list will be sent in input to the cosmopower_NN class, which will use this information: to derive the number of parameters in our model, equal to the number of elements in model_parameters . This number also corresponds to the number of nodes in the input layer of the neural network; to free the user from the burden of having to manually perform any ordering of the input parameters. The latter point guarantees flexibility and simplicity while using cosmopower_NN : to obtain predictions for a set of parameters, the user simply needs to feed a Python dict to cosmopower_NN , without having to worry about the ordering of the input parameters. For example, if I wanted to know the neural network prediction for a single set of parameters, I would collect them in the following dict : example_single_set_input_parameters = { 'n_s' : [ 0.96 ], 'h' : [ 0.7 ], 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.13 ], 'tau_reio' : [ 0.06 ], 'ln10^ {10} A_s' : [ 3.07 ], } Similarly, if I wanted to ask cosmopower_NN for e.g. 3 predictions, for 3 parameter sets, I would use: example_multiple_sets_input_parameters = { 'n_s' : np . array ([ 0.96 , 0.95 , 0.97 ]), 'h' : np . array ([ 0.7 , 0.64 , 0.72 ]), 'omega_b' : np . array ([ 0.0225 , 0.0226 , 0.0213 ]), 'omega_cdm' : np . array ([ 0.11 , 0.13 , 0.12 ]), 'tau_reio' : np . array ([ 0.07 , 0.06 , 0.08 ]), 'ln10^ {10} A_s' : np . array ([ 2.97 , 3.07 , 3.04 ]), } The possibility of asking cosmopower_NN for batch predictions, in particular, makes it a particularly useful tool to integrate within samplers that allow for batch evaluations of the likelihood. MODES \u00b6 A second, important piece of information for the cosmopower_NN class is the number of its output nodes, which corresponds to the number of sampled Fourier modes in our (log)-spectra, i.e. the number of multipoles \\(\\ell\\) for the CMB spectra, or the number of \\(k\\) -modes for the matter power spectrum. In the example of this notebook , we will emulate all of the \\(\\ell\\) multipoles between 2 and 2508. Note that we exclude \\(\\ell=0,1\\) as these are always 0. We can read the sampled \\(\\ell\\) range from the modes entry of our training_features dict (previously loaded): ell_range = training_features [ 'modes' ] print ( 'ell range: ' , ell_range ) ell range: [ 2 3 4 ... 2506 2507 2508] CLASS INSTANTIATION \u00b6 Finally, let\u2019s feed the information on model parameters and number of outputs to the cosmopower_NN class, which we instantiate with the following: from cosmopower import cosmopower_NN cp_nn = cosmopower_NN ( parameters = model_parameters , modes = ell_range , n_hidden = [ 512 , 512 , 512 , 512 ], # 4 hidden layers, each with 512 nodes verbose = True , # useful to understand the different steps in initialisation and training ) Initialized cosmopower_NN model, mapping 6 input parameters to 2507 output modes, using 4 hidden layers, with [512, 512, 512, 512] nodes, respectively. TRAINING \u00b6 This section shows how to train our model. To do this, we will call the method train() from the cosmopower_NN class. Here are the input arguments for this function: training_parameters : as explained above, this is a dict of np.arrays of input parameters. Each dict key has a np.array of values, for example training_parameters = { ` omega_b ` : np . array ([ 0.0222 , 0.0213 , 0.0241 , ... ]), ` omega_cdm ` : np . array ([ 0.114 , 0.134 , 0.124 , ... ]), ... etc ... } (each np.array has the same number of elements, equal to the number of training samples); training_features : a np.array of training (log)-power spectra. Its dimensions are (number of training samples, number of Fourier modes); filename_saved_model : path (without suffix) to the .pkl file where the trained model will be saved; validation split : float between 0 and 1, percentage of samples from the training set that will be used for validation. Some of the input arguments allow for the implementation of a learning schedule with different steps, each characterised by a different learning rate. In addition to the learning rate, the user can change other hyperparameters, such as the batch size, at each learning step: learning_rates : list of float values, the optimizer learning rate at each learning step; batch_sizes : list of float values, batch size for each learning step; gradient_accumulation_steps : list of int values, the number of accumulation batches if using gradient accumulation, at each learning step. Set these numbers to > 1 to activate gradient accumulation - only worth doing if using very large batch sizes; patience_values : list of int values, the number of epochs to wait for early stopping, at each learning step; max_epochs : list of int values, the maximum number of epochs for each learning step. with tf . device ( device ): # train cp_nn . train ( training_parameters = training_parameters , training_features = training_log_spectra , filename_saved_model = 'TT_cp_NN_example' , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ) After running this cell, the model is trained and, if we are satisfied with its performance on the testing set (see next section), we can download the .pkl file where the model was saved (given by the filename_saved_model argument of the train method of the cosmopower_NN class). TESTING \u00b6 To compute the predictions of the trained model on the testing set, we start by loading the trained model cp_nn = cosmopower_NN ( restore = True , restore_filename = 'TT_cp_NN_example' , ) Let\u2019s compute the predictions for the testing parameters. Note that we use the function ten_to_predictions_np , which, given input parameters, first performs forward passes of the trained model and then computes 10^ these predictions. Since we trained our model to emulate log-spectra , the output of ten_to_predictions_np is then given by predicted spectra . NOTE 1 : If we had trained our model to emulate spectra (without log), then to obtain predictions we would have used the function predictions_np . NOTE 2 : the functions ten_to_predictions_np and predictions_np implement forward passes through the network using only Numpy. These are optimised to be used in pipelines developed in simple Python code (not pure-Tensorflow pipelines). Conversely, the functions ten_to_predictions_tf and predictions_tf are pure-Tensorflow implementations, and as such are optimised to be used in pure-Tensorflow pipelines. NOTE 3 : both pure-Tensorflow and Numpy implementations allow for batch evaluation of input parameters - here, in particular, we can obtain predictions for all of the testing_parameters in just a single batch evaluation. predicted_testing_spectra = cp_nn . ten_to_predictions_np ( testing_params ) Plot examples of power spectra predictions from the testing set from matplotlib import gridspec fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 50 , 10 )) for i in range ( 3 ): pred = predicted_testing_spectra [ i ] * ell_range * ( ell_range + 1 ) / ( 2. * np . pi ) true = testing_spectra [ i ] * ell_range * ( ell_range + 1 ) / ( 2. * np . pi ) ax [ i ] . semilogx ( ell_range , true , 'blue' , label = 'Original' ) ax [ i ] . semilogx ( ell_range , pred , 'red' , label = 'NN reconstructed' , linestyle = '--' ) ax [ i ] . set_xlabel ( '$\\ell$' , fontsize = 'x-large' ) ax [ i ] . set_ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = 'x-large' ) ax [ i ] . legend ( fontsize = 15 ) We want to plot accuracy in units of Simons Observatory (SO) noise curves. Start by loading SO noise curves from the SO repository ! git clone https : // github . com / simonsobs / so_noise_models noise_levels_load = np . loadtxt ( './so_noise_models/LAT_comp_sep_noise/v3.1.0/SO_LAT_Nell_T_atmv1_goal_fsky0p4_ILC_CMB.txt' ) conv_factor = ( 2.7255e6 ) ** 2 ells = noise_levels_load [:, 0 ] SO_TT_noise = noise_levels_load [:, 1 ][: 2509 - 40 ] / conv_factor new_ells = ells [: 2509 - 40 ] f_sky = 0.4 prefac = np . sqrt ( 2 / ( f_sky * ( 2 * new_ells + 1 ))) denominator = prefac * ( testing_spectra [:, 38 :] + SO_TT_noise ) # use all of them diff = np . abs (( predicted_testing_spectra [:, 38 :] - testing_spectra [:, 38 :]) / ( denominator )) percentiles = np . zeros (( 4 , diff . shape [ 1 ])) percentiles [ 0 ] = np . percentile ( diff , 68 , axis = 0 ) percentiles [ 1 ] = np . percentile ( diff , 95 , axis = 0 ) percentiles [ 2 ] = np . percentile ( diff , 99 , axis = 0 ) percentiles [ 3 ] = np . percentile ( diff , 99.9 , axis = 0 ) plt . figure ( figsize = ( 12 , 9 )) plt . fill_between ( new_ells , 0 , percentiles [ 2 ,:], color = 'salmon' , label = '99%' , alpha = 0.8 ) plt . fill_between ( new_ells , 0 , percentiles [ 1 ,:], color = 'red' , label = '95%' , alpha = 0.7 ) plt . fill_between ( new_ells , 0 , percentiles [ 0 ,:], color = 'darkred' , label = '68%' , alpha = 1 ) plt . ylim ( 0 , 0.2 ) plt . legend ( frameon = False , fontsize = 30 , loc = 'upper left' ) plt . ylabel ( r '$\\frac{| C_{\\ell, \\rm {emulated} }^{\\rm {TT} } - C_{\\ell, \\rm {true} }^{\\rm {TT} }|} {\\sigma_{\\ell, \\rm {CMB} }^{\\rm {TT} }}$' , fontsize = 50 ) plt . xlabel ( r '$\\ell$' , fontsize = 50 ) ax = plt . gca () ax . xaxis . set_major_locator ( plt . MaxNLocator ( 10 )) ax . yaxis . set_major_locator ( plt . MaxNLocator ( 5 )) plt . setp ( ax . get_xticklabels (), fontsize = 25 ) plt . setp ( ax . get_yticklabels (), fontsize = 25 ) plt . tight_layout ()","title":"NN training"},{"location":"tutorial/training/NN/training_NN/#cosmopower_nn-instantiation","text":"We will now create an instance of the cosmopower_NN class . In order to instantiate the class, we need to first define some of the key aspects of our model.","title":"cosmopower_NN INSTANTIATION"},{"location":"tutorial/training/NN/training_NN/#training","text":"This section shows how to train our model. To do this, we will call the method train() from the cosmopower_NN class. Here are the input arguments for this function: training_parameters : as explained above, this is a dict of np.arrays of input parameters. Each dict key has a np.array of values, for example training_parameters = { ` omega_b ` : np . array ([ 0.0222 , 0.0213 , 0.0241 , ... ]), ` omega_cdm ` : np . array ([ 0.114 , 0.134 , 0.124 , ... ]), ... etc ... } (each np.array has the same number of elements, equal to the number of training samples); training_features : a np.array of training (log)-power spectra. Its dimensions are (number of training samples, number of Fourier modes); filename_saved_model : path (without suffix) to the .pkl file where the trained model will be saved; validation split : float between 0 and 1, percentage of samples from the training set that will be used for validation. Some of the input arguments allow for the implementation of a learning schedule with different steps, each characterised by a different learning rate. In addition to the learning rate, the user can change other hyperparameters, such as the batch size, at each learning step: learning_rates : list of float values, the optimizer learning rate at each learning step; batch_sizes : list of float values, batch size for each learning step; gradient_accumulation_steps : list of int values, the number of accumulation batches if using gradient accumulation, at each learning step. Set these numbers to > 1 to activate gradient accumulation - only worth doing if using very large batch sizes; patience_values : list of int values, the number of epochs to wait for early stopping, at each learning step; max_epochs : list of int values, the maximum number of epochs for each learning step. with tf . device ( device ): # train cp_nn . train ( training_parameters = training_parameters , training_features = training_log_spectra , filename_saved_model = 'TT_cp_NN_example' , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ) After running this cell, the model is trained and, if we are satisfied with its performance on the testing set (see next section), we can download the .pkl file where the model was saved (given by the filename_saved_model argument of the train method of the cosmopower_NN class).","title":"TRAINING"},{"location":"tutorial/training/NN/training_NN/#testing","text":"To compute the predictions of the trained model on the testing set, we start by loading the trained model cp_nn = cosmopower_NN ( restore = True , restore_filename = 'TT_cp_NN_example' , ) Let\u2019s compute the predictions for the testing parameters. Note that we use the function ten_to_predictions_np , which, given input parameters, first performs forward passes of the trained model and then computes 10^ these predictions. Since we trained our model to emulate log-spectra , the output of ten_to_predictions_np is then given by predicted spectra . NOTE 1 : If we had trained our model to emulate spectra (without log), then to obtain predictions we would have used the function predictions_np . NOTE 2 : the functions ten_to_predictions_np and predictions_np implement forward passes through the network using only Numpy. These are optimised to be used in pipelines developed in simple Python code (not pure-Tensorflow pipelines). Conversely, the functions ten_to_predictions_tf and predictions_tf are pure-Tensorflow implementations, and as such are optimised to be used in pure-Tensorflow pipelines. NOTE 3 : both pure-Tensorflow and Numpy implementations allow for batch evaluation of input parameters - here, in particular, we can obtain predictions for all of the testing_parameters in just a single batch evaluation. predicted_testing_spectra = cp_nn . ten_to_predictions_np ( testing_params ) Plot examples of power spectra predictions from the testing set from matplotlib import gridspec fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 50 , 10 )) for i in range ( 3 ): pred = predicted_testing_spectra [ i ] * ell_range * ( ell_range + 1 ) / ( 2. * np . pi ) true = testing_spectra [ i ] * ell_range * ( ell_range + 1 ) / ( 2. * np . pi ) ax [ i ] . semilogx ( ell_range , true , 'blue' , label = 'Original' ) ax [ i ] . semilogx ( ell_range , pred , 'red' , label = 'NN reconstructed' , linestyle = '--' ) ax [ i ] . set_xlabel ( '$\\ell$' , fontsize = 'x-large' ) ax [ i ] . set_ylabel ( '$ \\\\ frac{\\ell(\\ell+1)}{2 \\pi} C_\\ell$' , fontsize = 'x-large' ) ax [ i ] . legend ( fontsize = 15 ) We want to plot accuracy in units of Simons Observatory (SO) noise curves. Start by loading SO noise curves from the SO repository ! git clone https : // github . com / simonsobs / so_noise_models noise_levels_load = np . loadtxt ( './so_noise_models/LAT_comp_sep_noise/v3.1.0/SO_LAT_Nell_T_atmv1_goal_fsky0p4_ILC_CMB.txt' ) conv_factor = ( 2.7255e6 ) ** 2 ells = noise_levels_load [:, 0 ] SO_TT_noise = noise_levels_load [:, 1 ][: 2509 - 40 ] / conv_factor new_ells = ells [: 2509 - 40 ] f_sky = 0.4 prefac = np . sqrt ( 2 / ( f_sky * ( 2 * new_ells + 1 ))) denominator = prefac * ( testing_spectra [:, 38 :] + SO_TT_noise ) # use all of them diff = np . abs (( predicted_testing_spectra [:, 38 :] - testing_spectra [:, 38 :]) / ( denominator )) percentiles = np . zeros (( 4 , diff . shape [ 1 ])) percentiles [ 0 ] = np . percentile ( diff , 68 , axis = 0 ) percentiles [ 1 ] = np . percentile ( diff , 95 , axis = 0 ) percentiles [ 2 ] = np . percentile ( diff , 99 , axis = 0 ) percentiles [ 3 ] = np . percentile ( diff , 99.9 , axis = 0 ) plt . figure ( figsize = ( 12 , 9 )) plt . fill_between ( new_ells , 0 , percentiles [ 2 ,:], color = 'salmon' , label = '99%' , alpha = 0.8 ) plt . fill_between ( new_ells , 0 , percentiles [ 1 ,:], color = 'red' , label = '95%' , alpha = 0.7 ) plt . fill_between ( new_ells , 0 , percentiles [ 0 ,:], color = 'darkred' , label = '68%' , alpha = 1 ) plt . ylim ( 0 , 0.2 ) plt . legend ( frameon = False , fontsize = 30 , loc = 'upper left' ) plt . ylabel ( r '$\\frac{| C_{\\ell, \\rm {emulated} }^{\\rm {TT} } - C_{\\ell, \\rm {true} }^{\\rm {TT} }|} {\\sigma_{\\ell, \\rm {CMB} }^{\\rm {TT} }}$' , fontsize = 50 ) plt . xlabel ( r '$\\ell$' , fontsize = 50 ) ax = plt . gca () ax . xaxis . set_major_locator ( plt . MaxNLocator ( 10 )) ax . yaxis . set_major_locator ( plt . MaxNLocator ( 5 )) plt . setp ( ax . get_xticklabels (), fontsize = 25 ) plt . setp ( ax . get_yticklabels (), fontsize = 25 ) plt . tight_layout ()","title":"TESTING"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/","text":"This section is an excerpt from the notebook explaining how to train a cosmopower_PCAplusNN model . The user is strongly encouraged to run that notebook in its entirety, . Here we will consider only the most important parts to understand how the training works. PARAMETER FILES \u00b6 Let\u2019s take a look at the content of the parameter files that are read by CosmoPower during training. training_parameters_1 = np . load ( './camb_pp_training_params_1.npz' ) training_parameters_1 is a dict of np.arrays . There is a dict key for each of the parameters the emulator is going to be trained on: print ( training_parameters_1 . files ) ['omega_b', 'omega_cdm', 'h', 'tau_reio', 'n_s', 'ln10^{10}A_s'] Each of these keys has an associated np.array of values. The length of these arrays is the number of training samples in that file. Each file may have a different number of samples; their total number is the total number of samples in our training set. For example, let\u2019s check how many training samples are contained in the file we loaded: print ( training_parameters_1 [ 'omega_b' ]) print ( 'number of training samples in this file: ' , len ( training_parameters_1 [ 'omega_b' ])) # same for all of the other parameters [0.03959524 0.03194429 0.03467569 ... 0.03080418 0.01733493 0.03203036] number of training samples in this file: 15000 FEATURE FILES \u00b6 Now let\u2019s take a look at the \u201cfeatures\u201d. With features here we refer to the predictions of the neural network: these may be spectra or log-spectra values. In this case we are emulating values of the \\(\\phi \\phi\\) log -power spectra. The corresponding .npz files contain a dict with two keys: training_features_1 = np . load ( './camb_pp_training_log_spectra_1.npz' ) print ( training_features_1 . files ) ['modes', 'features'] The first key, modes , contains a np.array of the sampled Fourier modes. In this CMB case, the modes are the sampled \\(\\ell\\) multipoles; in the matter power spectrum case, modes would contain the sampled \\(k\\) -modes. In this specific example the log-power spectra are sampled at each multipole \\(\\ell\\) in the range \\([2, \\dots 2508]\\) , hence each training log-spectrum is a 2507-dimensional array. print ( training_features_1 [ 'modes' ]) print ( 'number of multipoles: ' , len ( training_features_1 [ 'modes' ])) [ 2 3 4 ... 2506 2507 2508] number of multipoles: 2507 The second key, features , has values equal to the actual values of the spectra. These are collected in a np.array of shape (number of training samples in this file , number of Fourier modes): training_log_spectra_1 = training_features_1 [ 'features' ] print ( '(number of training samples in this file, number of ell modes): ' , training_log_spectra_1 . shape ) (number of training samples in this file, number of ell modes): (15000, 2507) All of the other training features files have the same type of content, although the number of samples in each file may differ. The total number of samples in all of the training files represents the size of our training set. The files for the testing samples also have the same type of content as the training ones. testing_params = np . load ( './camb_pp_testing_params.npz' ) testing_spectra = 10. ** np . load ( './camb_pp_testing_log_spectra.npz' )[ 'features' ] cosmopower_PCA INSTANTIATION \u00b6 We will now create an instance of the cosmopower_PCA class, which will be used to perform PCA compression of the training spectra. This instance of the cosmopower_PCA will then be passed as an input to the cosmopower_PCAplusNN class, which implements the actual emulator of PCA components. In order to instantiate the cosmopower_PCA class, we will first need to define some of the key aspects of our model. PARAMETERS \u00b6 Let\u2019s start by defining the parameters of our model. If, for example, we want to emulate over a set of 6 standard \\(\\Lambda\\) CDM parameters, \\(\\omega_{\\mathrm{b}}, \\omega_{\\mathrm{cdm}}, h, \\tau, n_s, \\ln10^{10}A_s\\) we need to create a list with the names of all of these parameters, in arbitrary order: # list of parameter names, in arbitrary order model_parameters = [ 'h' , 'tau_reio' , 'omega_b' , 'n_s' , 'ln10^ {10} A_s' , 'omega_cdm' , ] This list will be sent in input to cosmopower_PCA . In turn, a cosmopower_PCA instance will be fed to the cosmopower_PCAplusNN class, which will use the information on the parameters of our model: to derive the number of parameters in our model, equal to the number of elements in model_parameters . This number also corresponds to the number of nodes in the input layer of the neural network; to free the user from the burden of having to manually perform any ordering of the input parameters. The latter point guarantees flexibility and simplicity while using cosmopower_PCAplusNN : to obtain predictions for a set of parameters, the user simply needs to feed a Python dict to cosmopower_PCAplusNN , without having to worry about the ordering of the input parameters. For example, if I wanted to know the neural network prediction for a single set of parameters, I would collect them in the following dict : example_single_set_input_parameters = { 'n_s' : [ 0.96 ], 'h' : [ 0.7 ], 'omega_b' : [ 0.0225 ], 'omega_cdm' : [ 0.13 ], 'tau_reio' : [ 0.06 ], 'ln10^ {10} A_s' : [ 3.07 ], } Similarly, if I wanted to ask cosmopower_PCAplusNN for e.g. the predictions for 3 parameter sets, I would use: example_multiple_sets_input_parameters = { 'n_s' : np . array ([ 0.96 , 0.95 , 0.97 ]), 'h' : np . array ([ 0.7 , 0.64 , 0.72 ]), 'omega_b' : np . array ([ 0.0225 , 0.0226 , 0.0213 ]), 'omega_cdm' : np . array ([ 0.11 , 0.13 , 0.12 ]), 'tau_reio' : np . array ([ 0.07 , 0.06 , 0.08 ]), 'ln10^ {10} A_s' : np . array ([ 2.97 , 3.07 , 3.04 ]), } The possibility of computing these batch predictions is particularly useful when cosmopower is integrated within samples that allow for batch evaluations of the likelihood. MODES \u00b6 A second, important piece of information for the cosmopower_PCA class is the number of sampled Fourier modes in our spectra, i.e. the number of multipoles \\(\\ell\\) for the CMB spectra, or the number of \\(k\\) -modes for the matter power spectrum. In this example, we will emulate all of the \\(\\ell\\) multipoles between 2 and 2508. Note that we exclude \\(\\ell=0,1\\) as these are always 0. We can read the sampled \\(\\ell\\) range from the modes entry of our training_features_1 dict (previously loaded): ell_range = training_features_1 [ 'modes' ] print ( 'ell range: ' , ell_range ) ell range: [ 2 3 4 ... 2506 2507 2508] NUMBER OF PCA COMPONENTS \u00b6 We also need to inform cosmopower_PCA of how many PCA components we want to keep: n_pcas = 64 CLASS INSTANTIATION \u00b6 Finally, let\u2019s feed the information on model parameters, modes and number of PCA coefficients to the cosmopower_PCA class. We also need to input a list of - parameters_filenames : collecting all of the files with parameter samples from the training set; - features_filenames : collecting all of the files with feature samples from the training set. parameters_filenames = [ './camb_pp_training_params_1' , './camb_pp_training_params_2' , './camb_pp_training_params_3' ] features_filenames = [ './camb_pp_training_log_spectra_1' , './camb_pp_training_log_spectra_2' , './camb_pp_training_log_spectra_3' ] from cosmopower import cosmopower_PCA cp_pca = cosmopower_PCA ( parameters = model_parameters , modes = ell_range , n_pcas = n_pcas , parameters_filenames = parameters_filenames , features_filenames = features_filenames , verbose = True , # useful to follow the various steps ) Initialized cosmopower_PCA compression with 64 components PCA COMPRESSION \u00b6 The transform_and_stack_training_data method of the cosmopower_PCA class runs the full compression pipeline on the training data. Note that cosmopower_PCA internally performs IncrementalPCA , allowing it to process input data in batches - this is why we did not collect the training data in one single file. Leaving the input data in multiple files allows for a reduced computational burden in terms of memory requirements. cp_pca . transform_and_stack_training_data () cosmopower_PCAplusNN INSTANTIATION \u00b6 After having performed PCA compression of the training set with the cosmopower_PCA class, we can now feed this class to the cosmopower_PCAplusNN class. The latter represents the actual emulator, mapping parameters onto PCA coefficients. Hence, in initialising cosmopower_PCAplusNN we can decide on details of the network architecture: from cosmopower import cosmopower_PCAplusNN cp_pca_nn = cosmopower_PCAplusNN ( cp_pca = cp_pca , n_hidden = [ 512 , 512 , 512 , 512 ], # 4 hidden layers, each with 512 nodes verbose = True , # useful to understand the different steps in initialisation and training ) Initialized cosmopower_PCAplusNN model, mapping 6 input parameters to 64 PCA components and then inverting the PCA compression to obtain 2507 modes The model uses 4 hidden layers, with [512, 512, 512, 512] nodes, respectively. TRAINING \u00b6 In this section we will train our model. To do this, we will call the method train() from the cosmopower_PCAplusNN class. Here are the input arguments for this function: filename_saved_model : path (without suffix) to the .pkl file where the trained model will be saved; validation split : float between 0 and 1, percentage of samples from the training set that will be used for validation. Some of the input arguments allow for the implementation of a learning schedule with different steps, each characterised by a different learning rate. In addition to the learning rate, the user can change other hyperparameters, such as the batch size, at each learning step: learning_rates : list of float values, the optimizer learning rate at each learning step; batch_sizes : list of float values, batch size for each learning step; gradient_accumulation_steps : list of int values, the number of accumulation batches if using gradient accumulation, at each learning step. Set these numbers to > 1 to activate gradient accumulation - only worth doing if using very large batch sizes; patience_values : list of int values, the number of epochs to wait for early stopping, at each learning step; max_epochs : list of int values, the maximum number of epochs for each learning step. with tf . device ( device ): # train cp_pca_nn . train ( filename_saved_model = 'PP_cp_PCAplusNN_example' , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ) The model is trained! If you are satisfied with its performance on the testing set (see next section) you can download the .pkl file where the model was saved (given by the filename_saved_model argument of the train method of the cosmopower_PCAplusNN class). TESTING \u00b6 Let\u2019s load the trained model cp_pca_nn = cosmopower_PCAplusNN ( restore = True , restore_filename = 'PP_cp_PCAplusNN_example' ) Let\u2019s compute the predictions for the testing parameters. Note that we use the function ten_to_predictions_np , which, given input parameters, first performs forward passes of the trained model and then computes 10^ these predictions. Since we trained our model to emulate log-spectra , the output of ten_to_predictions_np is then given by predicted spectra . NOTE 1 : If we had trained our model to emulate spectra (without log), then to obtain predictions we would have used the function predictions_np . NOTE 2 : the functions ten_to_predictions_np and predictions_np implement forward passes through the network using only Numpy. These are optimised to be used in pipelines developed in simple Python code (not pure-Tensorflow pipelines). Conversely, the functions ten_to_predictions_tf and predictions_tf are pure-Tensorflow implementations, and as such are optimised to be used in pure-Tensorflow pipelines. NOTE 3 : both pure-Tensorflow and Numpy implementations allow for batch evaluation of input parameters - here, in particular, we can obtain predictions for all of the testing_parameters in just a single batch evaluation. predicted_testing_spectra = cp_pca_nn . ten_to_predictions_np ( testing_params ) Plot examples of power spectra predictions from the testing set fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 50 , 10 )) for i in range ( 3 ): pred = predicted_testing_spectra [ i ] * ell_range * ell_range * ( ell_range + 1 ) * ( ell_range + 1 ) / ( 2. * np . pi ) true = testing_spectra [ i ] * ell_range * ell_range * ( ell_range + 1 ) * ( ell_range + 1 ) / ( 2. * np . pi ) ax [ i ] . semilogx ( ell_range , true , 'blue' , label = 'Original' ) ax [ i ] . semilogx ( ell_range , pred , 'red' , label = 'NN reconstructed' , linestyle = '--' ) ax [ i ] . set_xlabel ( '$\\ell$' , fontsize = 'x-large' ) ax [ i ] . set_ylabel ( '$ \\\\ frac{[\\ell(\\ell+1)]^2}{2 \\pi} C_\\ell$' , fontsize = 'x-large' ) ax [ i ] . legend ( fontsize = 15 ) We want to plot accuracy in units of Simons Observatory (SO) noise curves. Start by loading SO noise curves from the SO repository ! git clone https : // github . com / simonsobs / so_noise_models # load noise models from the SO noise repo noise_levels_load = np . loadtxt ( './so_noise_models/LAT_lensing_noise/lensing_v3_1_0/nlkk_v3_1_0deproj0_SENS1_fsky0p4_it_lT30-3000_lP30-5000.dat' ) ells = noise_levels_load [:, 0 ] SO_PP_MV_noise = noise_levels_load [:, 7 ][: 2507 ] new_ells = ells [: 2507 ] # 2509 is the final for our spectra, starting from 2 SO_PP_MV_noise /= ( new_ells * ( new_ells + 1 ) / 2 ) ** 2 # convert from kappa_kappa to phi_phi f_sky = 0.4 prefac = np . sqrt ( 2 / ( f_sky * ( 2 * new_ells + 1 ))) denominator = prefac * ( testing_spectra + SO_PP_MV_noise ) # use all of them diff = np . abs (( testing_spectra - predicted_testing_spectra ) / ( denominator )) percentiles = np . zeros (( 4 , diff . shape [ 1 ])) percentiles [ 0 ] = np . percentile ( diff , 68 , axis = 0 ) percentiles [ 1 ] = np . percentile ( diff , 95 , axis = 0 ) percentiles [ 2 ] = np . percentile ( diff , 99 , axis = 0 ) percentiles [ 3 ] = np . percentile ( diff , 99.9 , axis = 0 ) plt . figure ( figsize = ( 12 , 9 )) plt . fill_between ( new_ells , 0 , percentiles [ 2 ,:], color = 'salmon' , label = '99%' , alpha = 0.8 ) plt . fill_between ( new_ells , 0 , percentiles [ 1 ,:], color = 'red' , label = '95%' , alpha = 0.7 ) plt . fill_between ( new_ells , 0 , percentiles [ 0 ,:], color = 'darkred' , label = '68%' , alpha = 1 ) plt . ylim ( 0 , 0.25 ) plt . legend ( frameon = False , fontsize = 30 , loc = 'upper right' ) plt . ylabel ( r '$\\frac{| C_{\\ell, \\rm {emulated} }^{\\phi \\phi} - C_{\\ell, \\rm {true} }^{\\phi \\phi}|} {\\sigma_{\\ell, \\rm {CMB} }^{\\phi \\phi}}$' , fontsize = 50 ) plt . xlabel ( r '$\\ell$' , fontsize = 50 ) ax = plt . gca () ax . xaxis . set_major_locator ( plt . MaxNLocator ( 10 )) ax . yaxis . set_major_locator ( plt . MaxNLocator ( 5 )) plt . setp ( ax . get_xticklabels (), fontsize = 25 ) plt . setp ( ax . get_yticklabels (), fontsize = 25 ) plt . tight_layout ()","title":"PCA+NN training"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/#cosmopower_pca-instantiation","text":"We will now create an instance of the cosmopower_PCA class, which will be used to perform PCA compression of the training spectra. This instance of the cosmopower_PCA will then be passed as an input to the cosmopower_PCAplusNN class, which implements the actual emulator of PCA components. In order to instantiate the cosmopower_PCA class, we will first need to define some of the key aspects of our model.","title":"cosmopower_PCA INSTANTIATION"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/#pca-compression","text":"The transform_and_stack_training_data method of the cosmopower_PCA class runs the full compression pipeline on the training data. Note that cosmopower_PCA internally performs IncrementalPCA , allowing it to process input data in batches - this is why we did not collect the training data in one single file. Leaving the input data in multiple files allows for a reduced computational burden in terms of memory requirements. cp_pca . transform_and_stack_training_data ()","title":"PCA COMPRESSION"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/#cosmopower_pcaplusnn-instantiation","text":"After having performed PCA compression of the training set with the cosmopower_PCA class, we can now feed this class to the cosmopower_PCAplusNN class. The latter represents the actual emulator, mapping parameters onto PCA coefficients. Hence, in initialising cosmopower_PCAplusNN we can decide on details of the network architecture: from cosmopower import cosmopower_PCAplusNN cp_pca_nn = cosmopower_PCAplusNN ( cp_pca = cp_pca , n_hidden = [ 512 , 512 , 512 , 512 ], # 4 hidden layers, each with 512 nodes verbose = True , # useful to understand the different steps in initialisation and training ) Initialized cosmopower_PCAplusNN model, mapping 6 input parameters to 64 PCA components and then inverting the PCA compression to obtain 2507 modes The model uses 4 hidden layers, with [512, 512, 512, 512] nodes, respectively.","title":"cosmopower_PCAplusNN INSTANTIATION"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/#training","text":"In this section we will train our model. To do this, we will call the method train() from the cosmopower_PCAplusNN class. Here are the input arguments for this function: filename_saved_model : path (without suffix) to the .pkl file where the trained model will be saved; validation split : float between 0 and 1, percentage of samples from the training set that will be used for validation. Some of the input arguments allow for the implementation of a learning schedule with different steps, each characterised by a different learning rate. In addition to the learning rate, the user can change other hyperparameters, such as the batch size, at each learning step: learning_rates : list of float values, the optimizer learning rate at each learning step; batch_sizes : list of float values, batch size for each learning step; gradient_accumulation_steps : list of int values, the number of accumulation batches if using gradient accumulation, at each learning step. Set these numbers to > 1 to activate gradient accumulation - only worth doing if using very large batch sizes; patience_values : list of int values, the number of epochs to wait for early stopping, at each learning step; max_epochs : list of int values, the maximum number of epochs for each learning step. with tf . device ( device ): # train cp_pca_nn . train ( filename_saved_model = 'PP_cp_PCAplusNN_example' , # cooling schedule validation_split = 0.1 , learning_rates = [ 1e-2 , 1e-3 , 1e-4 , 1e-5 , 1e-6 ], batch_sizes = [ 1024 , 1024 , 1024 , 1024 , 1024 ], gradient_accumulation_steps = [ 1 , 1 , 1 , 1 , 1 ], # early stopping set up patience_values = [ 100 , 100 , 100 , 100 , 100 ], max_epochs = [ 1000 , 1000 , 1000 , 1000 , 1000 ], ) The model is trained! If you are satisfied with its performance on the testing set (see next section) you can download the .pkl file where the model was saved (given by the filename_saved_model argument of the train method of the cosmopower_PCAplusNN class).","title":"TRAINING"},{"location":"tutorial/training/PCAplusNN/training_PCAplusNN/#testing","text":"Let\u2019s load the trained model cp_pca_nn = cosmopower_PCAplusNN ( restore = True , restore_filename = 'PP_cp_PCAplusNN_example' ) Let\u2019s compute the predictions for the testing parameters. Note that we use the function ten_to_predictions_np , which, given input parameters, first performs forward passes of the trained model and then computes 10^ these predictions. Since we trained our model to emulate log-spectra , the output of ten_to_predictions_np is then given by predicted spectra . NOTE 1 : If we had trained our model to emulate spectra (without log), then to obtain predictions we would have used the function predictions_np . NOTE 2 : the functions ten_to_predictions_np and predictions_np implement forward passes through the network using only Numpy. These are optimised to be used in pipelines developed in simple Python code (not pure-Tensorflow pipelines). Conversely, the functions ten_to_predictions_tf and predictions_tf are pure-Tensorflow implementations, and as such are optimised to be used in pure-Tensorflow pipelines. NOTE 3 : both pure-Tensorflow and Numpy implementations allow for batch evaluation of input parameters - here, in particular, we can obtain predictions for all of the testing_parameters in just a single batch evaluation. predicted_testing_spectra = cp_pca_nn . ten_to_predictions_np ( testing_params ) Plot examples of power spectra predictions from the testing set fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 50 , 10 )) for i in range ( 3 ): pred = predicted_testing_spectra [ i ] * ell_range * ell_range * ( ell_range + 1 ) * ( ell_range + 1 ) / ( 2. * np . pi ) true = testing_spectra [ i ] * ell_range * ell_range * ( ell_range + 1 ) * ( ell_range + 1 ) / ( 2. * np . pi ) ax [ i ] . semilogx ( ell_range , true , 'blue' , label = 'Original' ) ax [ i ] . semilogx ( ell_range , pred , 'red' , label = 'NN reconstructed' , linestyle = '--' ) ax [ i ] . set_xlabel ( '$\\ell$' , fontsize = 'x-large' ) ax [ i ] . set_ylabel ( '$ \\\\ frac{[\\ell(\\ell+1)]^2}{2 \\pi} C_\\ell$' , fontsize = 'x-large' ) ax [ i ] . legend ( fontsize = 15 ) We want to plot accuracy in units of Simons Observatory (SO) noise curves. Start by loading SO noise curves from the SO repository ! git clone https : // github . com / simonsobs / so_noise_models # load noise models from the SO noise repo noise_levels_load = np . loadtxt ( './so_noise_models/LAT_lensing_noise/lensing_v3_1_0/nlkk_v3_1_0deproj0_SENS1_fsky0p4_it_lT30-3000_lP30-5000.dat' ) ells = noise_levels_load [:, 0 ] SO_PP_MV_noise = noise_levels_load [:, 7 ][: 2507 ] new_ells = ells [: 2507 ] # 2509 is the final for our spectra, starting from 2 SO_PP_MV_noise /= ( new_ells * ( new_ells + 1 ) / 2 ) ** 2 # convert from kappa_kappa to phi_phi f_sky = 0.4 prefac = np . sqrt ( 2 / ( f_sky * ( 2 * new_ells + 1 ))) denominator = prefac * ( testing_spectra + SO_PP_MV_noise ) # use all of them diff = np . abs (( testing_spectra - predicted_testing_spectra ) / ( denominator )) percentiles = np . zeros (( 4 , diff . shape [ 1 ])) percentiles [ 0 ] = np . percentile ( diff , 68 , axis = 0 ) percentiles [ 1 ] = np . percentile ( diff , 95 , axis = 0 ) percentiles [ 2 ] = np . percentile ( diff , 99 , axis = 0 ) percentiles [ 3 ] = np . percentile ( diff , 99.9 , axis = 0 ) plt . figure ( figsize = ( 12 , 9 )) plt . fill_between ( new_ells , 0 , percentiles [ 2 ,:], color = 'salmon' , label = '99%' , alpha = 0.8 ) plt . fill_between ( new_ells , 0 , percentiles [ 1 ,:], color = 'red' , label = '95%' , alpha = 0.7 ) plt . fill_between ( new_ells , 0 , percentiles [ 0 ,:], color = 'darkred' , label = '68%' , alpha = 1 ) plt . ylim ( 0 , 0.25 ) plt . legend ( frameon = False , fontsize = 30 , loc = 'upper right' ) plt . ylabel ( r '$\\frac{| C_{\\ell, \\rm {emulated} }^{\\phi \\phi} - C_{\\ell, \\rm {true} }^{\\phi \\phi}|} {\\sigma_{\\ell, \\rm {CMB} }^{\\phi \\phi}}$' , fontsize = 50 ) plt . xlabel ( r '$\\ell$' , fontsize = 50 ) ax = plt . gca () ax . xaxis . set_major_locator ( plt . MaxNLocator ( 10 )) ax . yaxis . set_major_locator ( plt . MaxNLocator ( 5 )) plt . setp ( ax . get_xticklabels (), fontsize = 25 ) plt . setp ( ax . get_yticklabels (), fontsize = 25 ) plt . tight_layout ()","title":"TESTING"},{"location":"tutorial/training/data_generation/create_params/","text":"To build a training/testing dataset we start by generating a Latin Hypercube Sampling grid of parameters. The power spectra predictions computed by a Boltzmann code at these nodes will represent our features dataset. Here we will show an example of producing such a grid with matter power spectrum parameters: the procedure is analogous in the CMB case. Generate Latin Hypercube Sampled parameters \u00b6 import numpy as np import pyDOE as pyDOE # number of parameters and samples n_params = 8 n_samples = 400000 # parameter ranges obh2 = np . linspace ( 0.01865 , 0.02625 , n_samples ) omch2 = np . linspace ( 0.05 , 0.255 , n_samples ) h = np . linspace ( 0.64 , 0.82 , n_samples ) ns = np . linspace ( 0.84 , 1.1 , n_samples ) lnAs = np . linspace ( 1.61 , 3.91 , n_samples ) cmin = np . linspace ( 2. , 4. , n_samples ) eta0 = np . linspace ( 0.5 , 1. , n_samples ) z = np . linspace ( 0 , 5. , n_samples ) # LHS grid AllParams = np . vstack ([ obh2 , omch2 , h , ns , lnAs , cmin , eta0 , z ]) lhd = pyDOE . lhs ( n_params , samples = n_samples , criterion = None ) idx = ( lhd * n_samples ) . astype ( int ) AllCombinations = np . zeros (( n_samples , n_params )) for i in range ( n_params ): AllCombinations [:, i ] = AllParams [ i ][ idx [:, i ]] # saving params = { 'omega_b' : AllCombinations [:, 0 ], 'omega_cdm' : AllCombinations [:, 1 ], 'h' : AllCombinations [:, 2 ], 'n_s' : AllCombinations [:, 3 ], 'ln10^ {10} A_s' : AllCombinations [:, 4 ], 'c_min' : AllCombinations [:, 5 ], 'eta_0' : AllCombinations [:, 6 ], 'z' : AllCombinations [:, 7 ], } np . savez ( 'your_LHS_parameter_file.npz' , ** params )","title":"Latin Hypercube Sampling the emulator parameters"},{"location":"tutorial/training/data_generation/create_params/#generate-latin-hypercube-sampled-parameters","text":"import numpy as np import pyDOE as pyDOE # number of parameters and samples n_params = 8 n_samples = 400000 # parameter ranges obh2 = np . linspace ( 0.01865 , 0.02625 , n_samples ) omch2 = np . linspace ( 0.05 , 0.255 , n_samples ) h = np . linspace ( 0.64 , 0.82 , n_samples ) ns = np . linspace ( 0.84 , 1.1 , n_samples ) lnAs = np . linspace ( 1.61 , 3.91 , n_samples ) cmin = np . linspace ( 2. , 4. , n_samples ) eta0 = np . linspace ( 0.5 , 1. , n_samples ) z = np . linspace ( 0 , 5. , n_samples ) # LHS grid AllParams = np . vstack ([ obh2 , omch2 , h , ns , lnAs , cmin , eta0 , z ]) lhd = pyDOE . lhs ( n_params , samples = n_samples , criterion = None ) idx = ( lhd * n_samples ) . astype ( int ) AllCombinations = np . zeros (( n_samples , n_params )) for i in range ( n_params ): AllCombinations [:, i ] = AllParams [ i ][ idx [:, i ]] # saving params = { 'omega_b' : AllCombinations [:, 0 ], 'omega_cdm' : AllCombinations [:, 1 ], 'h' : AllCombinations [:, 2 ], 'n_s' : AllCombinations [:, 3 ], 'ln10^ {10} A_s' : AllCombinations [:, 4 ], 'c_min' : AllCombinations [:, 5 ], 'eta_0' : AllCombinations [:, 6 ], 'z' : AllCombinations [:, 7 ], } np . savez ( 'your_LHS_parameter_file.npz' , ** params )","title":"Generate Latin Hypercube Sampled parameters"},{"location":"tutorial/training/data_generation/create_spectra/","text":"Now that we have our gird of parameters, we can use it to generate the corresponding spectra with e.g. the Boltzmann code Class , available for download here . Generating training spectra with Class \u00b6 import numpy as np import classy import sys krange1 = np . logspace ( np . log10 ( 1e-5 ), np . log10 ( 1e-4 ), num = 20 , endpoint = False ) krange2 = np . logspace ( np . log10 ( 1e-4 ), np . log10 ( 1e-3 ), num = 40 , endpoint = False ) krange3 = np . logspace ( np . log10 ( 1e-3 ), np . log10 ( 1e-2 ), num = 60 , endpoint = False ) krange4 = np . logspace ( np . log10 ( 1e-2 ), np . log10 ( 1e-1 ), num = 80 , endpoint = False ) krange5 = np . logspace ( np . log10 ( 1e-1 ), np . log10 ( 1 ), num = 100 , endpoint = False ) krange6 = np . logspace ( np . log10 ( 1 ), np . log10 ( 10 ), num = 120 , endpoint = False ) k = np . concatenate (( krange1 , krange2 , krange3 , krange4 , krange5 , krange6 )) num_k = len ( k ) # 420 k-modes np . savetxt ( 'k_modes.txt' , k ) cosmo = classy . Class () params_lhs = np . load ( 'your_LHS_parameter_file.npz' ) def spectra_generation ( i ): print ( 'parameter set ' , i ) # Define your cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl mPk' , 'non linear' : 'hmcode' , 'z_max_pk' : 5 , 'P_k_max_1/Mpc' : 10. , 'nonlinear_min_k_max' : 100. , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : params_lhs [ 'omega_b' ][ i ], 'omega_cdm' : params_lhs [ 'omega_cdm' ][ i ], 'h' : params_lhs [ 'h' ][ i ], 'n_s' : params_lhs [ 'n_s' ][ i ], 'ln10^ {10} A_s' : params_lhs [ 'ln10^ {10} A_s' ][ i ], 'c_min' : params_lhs [ 'c_min' ][ i ], 'eta_0' : params_lhs [ 'eta_0' ][ i ], } # Set the parameters to the cosmological code cosmo . set ( params ) try : cosmo . compute () z = params_lhs [ 'z' ][ i ] # non linear power spectrum Pnonlin = np . array ([ cosmo . pk ( ki , z ) for ki in k ]) # linear power spectrum Plin = np . array ([ cosmo . pk_lin ( ki , z ) for ki in k ]) cosmo_array = np . hstack (([ params_lhs [ k ][ i ] for k in params_lhs ], Plin )) f = open ( './linear.dat' , 'ab' ) np . savetxt ( f , [ cosmo_array ]) f . close () # non linear boost ratio = Pnonlin / Plin cosmo_array = np . hstack (([ params_lhs [ k ][ i ] for k in params_lhs ], ratio )) f = open ( './boost.dat' , 'ab' ) np . savetxt ( f , [ cosmo_array ]) f . close () # parameter set rejected by Class except classy . CosmoComputationError as failure_message : print ( str ( failure_message ) + ' \\n ' ) cosmo . struct_cleanup () cosmo . empty () # something wrong in Class init except classy . CosmoSevereError as critical_message : print ( \"Something went wrong when calling CLASS\" + str ( critical_message )) cosmo . struct_cleanup () cosmo . empty () return # loop over parameter sets for i in range ( len ( params_lhs [ 'omega_b' ])): spectra_generation ( i )","title":"Creating the spectra with a Boltzmann code"},{"location":"tutorial/training/data_generation/create_spectra/#generating-training-spectra-with-class","text":"import numpy as np import classy import sys krange1 = np . logspace ( np . log10 ( 1e-5 ), np . log10 ( 1e-4 ), num = 20 , endpoint = False ) krange2 = np . logspace ( np . log10 ( 1e-4 ), np . log10 ( 1e-3 ), num = 40 , endpoint = False ) krange3 = np . logspace ( np . log10 ( 1e-3 ), np . log10 ( 1e-2 ), num = 60 , endpoint = False ) krange4 = np . logspace ( np . log10 ( 1e-2 ), np . log10 ( 1e-1 ), num = 80 , endpoint = False ) krange5 = np . logspace ( np . log10 ( 1e-1 ), np . log10 ( 1 ), num = 100 , endpoint = False ) krange6 = np . logspace ( np . log10 ( 1 ), np . log10 ( 10 ), num = 120 , endpoint = False ) k = np . concatenate (( krange1 , krange2 , krange3 , krange4 , krange5 , krange6 )) num_k = len ( k ) # 420 k-modes np . savetxt ( 'k_modes.txt' , k ) cosmo = classy . Class () params_lhs = np . load ( 'your_LHS_parameter_file.npz' ) def spectra_generation ( i ): print ( 'parameter set ' , i ) # Define your cosmology (what is not specified will be set to CLASS default parameters) params = { 'output' : 'tCl mPk' , 'non linear' : 'hmcode' , 'z_max_pk' : 5 , 'P_k_max_1/Mpc' : 10. , 'nonlinear_min_k_max' : 100. , 'N_ncdm' : 0 , 'N_eff' : 3.046 , 'omega_b' : params_lhs [ 'omega_b' ][ i ], 'omega_cdm' : params_lhs [ 'omega_cdm' ][ i ], 'h' : params_lhs [ 'h' ][ i ], 'n_s' : params_lhs [ 'n_s' ][ i ], 'ln10^ {10} A_s' : params_lhs [ 'ln10^ {10} A_s' ][ i ], 'c_min' : params_lhs [ 'c_min' ][ i ], 'eta_0' : params_lhs [ 'eta_0' ][ i ], } # Set the parameters to the cosmological code cosmo . set ( params ) try : cosmo . compute () z = params_lhs [ 'z' ][ i ] # non linear power spectrum Pnonlin = np . array ([ cosmo . pk ( ki , z ) for ki in k ]) # linear power spectrum Plin = np . array ([ cosmo . pk_lin ( ki , z ) for ki in k ]) cosmo_array = np . hstack (([ params_lhs [ k ][ i ] for k in params_lhs ], Plin )) f = open ( './linear.dat' , 'ab' ) np . savetxt ( f , [ cosmo_array ]) f . close () # non linear boost ratio = Pnonlin / Plin cosmo_array = np . hstack (([ params_lhs [ k ][ i ] for k in params_lhs ], ratio )) f = open ( './boost.dat' , 'ab' ) np . savetxt ( f , [ cosmo_array ]) f . close () # parameter set rejected by Class except classy . CosmoComputationError as failure_message : print ( str ( failure_message ) + ' \\n ' ) cosmo . struct_cleanup () cosmo . empty () # something wrong in Class init except classy . CosmoSevereError as critical_message : print ( \"Something went wrong when calling CLASS\" + str ( critical_message )) cosmo . struct_cleanup () cosmo . empty () return # loop over parameter sets for i in range ( len ( params_lhs [ 'omega_b' ])): spectra_generation ( i )","title":"Generating training spectra with Class"},{"location":"tutorial/training/data_generation/format/","text":"Here we simply show how to format the parameter and (log)-spectra data so that they can be read by cosmopower_NN and cosmopower_PCAplusNN during training. Preparing the files for use in training \u00b6 import numpy as np # redefine k_modes = np . loadtxt ( 'k_modes.txt' ) # load predictions from Class: concatenations of parameters and spectra linear_spectra_and_params = np . loadtxt ( './linear.dat' ) boost_spectra_and_params = np . loadtxt ( './boost.dat' ) # clean NaN's if any rows = np . where ( np . isfinite ( linear_spectra_and_params ) . all ( 1 )) linear_spectra_and_params = linear_spectra_and_params [ rows ] rows = np . where ( np . isfinite ( boost_spectra_and_params ) . all ( 1 )) boost_spectra_and_params = boost_spectra_and_params [ rows ] # here the ordering should match the one used in `1_create_params.py` params = [ 'omega_b' , 'omega_cdm' , 'h' , 'n_s' , 'ln10^ {10} A_s' , 'c_min' , 'eta_0' , 'z' ] n_params = len ( params ) # separate parameters from spectra, take log linear_parameters = linear_spectra_and_params [:, : n_params ] linear_log_spectra = np . log10 ( linear_spectra_and_params [:, n_params :]) boost_parameters = boost_spectra_and_params [:, : n_params ] boost_log_spectra = np . log10 ( boost_spectra_and_params [:, n_params :]) linear_parameters_dict = { params [ i ]: linear_parameters [:, i ] for i in range ( len ( params ))} linear_log_spectra_dict = { 'modes' : k_modes , 'features' : linear_log_spectra } boost_parameters_dict = { params [ i ]: boost_parameters [:, i ] for i in range ( len ( params ))} boost_log_spectra_dict = { 'modes' : k_modes , 'features' : boost_log_spectra } # save np . savez ( 'class_linear_params.npz' , ** linear_parameters_dict ) np . savez ( 'class_linear_logpower.npz' , ** linear_log_spectra_dict ) np . savez ( 'class_boost_params.npz' , ** boost_parameters_dict ) np . savez ( 'class_boost_logpower.npz' , ** boost_log_spectra_dict )","title":"Preparing the data for training"},{"location":"tutorial/training/data_generation/format/#preparing-the-files-for-use-in-training","text":"import numpy as np # redefine k_modes = np . loadtxt ( 'k_modes.txt' ) # load predictions from Class: concatenations of parameters and spectra linear_spectra_and_params = np . loadtxt ( './linear.dat' ) boost_spectra_and_params = np . loadtxt ( './boost.dat' ) # clean NaN's if any rows = np . where ( np . isfinite ( linear_spectra_and_params ) . all ( 1 )) linear_spectra_and_params = linear_spectra_and_params [ rows ] rows = np . where ( np . isfinite ( boost_spectra_and_params ) . all ( 1 )) boost_spectra_and_params = boost_spectra_and_params [ rows ] # here the ordering should match the one used in `1_create_params.py` params = [ 'omega_b' , 'omega_cdm' , 'h' , 'n_s' , 'ln10^ {10} A_s' , 'c_min' , 'eta_0' , 'z' ] n_params = len ( params ) # separate parameters from spectra, take log linear_parameters = linear_spectra_and_params [:, : n_params ] linear_log_spectra = np . log10 ( linear_spectra_and_params [:, n_params :]) boost_parameters = boost_spectra_and_params [:, : n_params ] boost_log_spectra = np . log10 ( boost_spectra_and_params [:, n_params :]) linear_parameters_dict = { params [ i ]: linear_parameters [:, i ] for i in range ( len ( params ))} linear_log_spectra_dict = { 'modes' : k_modes , 'features' : linear_log_spectra } boost_parameters_dict = { params [ i ]: boost_parameters [:, i ] for i in range ( len ( params ))} boost_log_spectra_dict = { 'modes' : k_modes , 'features' : boost_log_spectra } # save np . savez ( 'class_linear_params.npz' , ** linear_parameters_dict ) np . savez ( 'class_linear_logpower.npz' , ** linear_log_spectra_dict ) np . savez ( 'class_boost_params.npz' , ** boost_parameters_dict ) np . savez ( 'class_boost_logpower.npz' , ** boost_log_spectra_dict )","title":"Preparing the files for use in training"}]}